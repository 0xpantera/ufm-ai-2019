{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El uso de funciones de activacion no lineares como la diferencia clave entre modelos lineales\n",
    "* Los diferentes tipos de funciones de activacion\n",
    "* El modulo `nn` de PyTorch que contiene los bloques para construir NNs\n",
    "* Resolver un problema simple de un _fit_ lineal con una NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronas artificiales\n",
    "\n",
    "* Neural networks: entidades matematicas capaces de representar funciones complicadas a traves de una composicion de funciones mas simples.\n",
    "* Originalmente inspiradas por la forma en la que funciona nuestro cerebro.\n",
    "* El bloque de construccion basico es una neurona:\n",
    "    * Esencialmente una transformacion linear del input (e.g. multiplicacion del input por un numero, el _weight_, y la suma de una constante, el _bias_.\n",
    "    * Seguido por la aplicacion de una funcion no lineal (referida como la funcion de activacion)\n",
    "    * $o = f(w x + b)$\n",
    "    * x es nuestro input, w el _weight_ y b el _bias_. $f$ es la funcion de activacion.\n",
    "    * x puede ser un escalar o un vector de valores, w puede ser un escalar o una matriz, mientras que b es un escalar o un vector.\n",
    "* La expresion $o = f(w x + b)$ es una capa de neuronas, ya que representa varias neuronas a traves de los _weights_ y _bias_ multidimensionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_1 = f(w_0 x_0 + b_0)$\n",
    "\n",
    "$x_2 = f(w_1 x_1 + b_1)$\n",
    "\n",
    "$...$\n",
    "\n",
    "$y = f(w_n x_n + b_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **dibujos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de activacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nuestro modelo anterior ya tenia una operacion lineal. Eso era el modelo entero.\n",
    "* El rol de la funcion de activacion es concentrar los _outputs_ de la operacion lineal precedente a un rango dado.\n",
    "* Si queremos asignar un _score_ al output del modelo necesitamos limitar el rango de numeros posibles para ese _score_\n",
    "    * `float32`\n",
    "    * $\\sum wx + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Que opciones tenemos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Una opcion seria ponerle un limite a los valores del _output_.\n",
    "    * Cualquier cosa debajo de cero seria cero\n",
    "    * cualquier cosa arriba de 10 seria 10\n",
    "    * `torch.nn.Hardtanh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9757431300314515"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "math.tanh(-2.2) # camion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09966799462495582"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.tanh(0.1) # oso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866142981514303"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.tanh(2.5) # perro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Funciones de activacion](../assets/activaciones.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hay muchas funciones de activacion.\n",
    "* Por definicion, las funciones de activacion:\n",
    "    * Son no lineales. Aplicaciones repetidas de $wx+b$ sin una funcion de activacion resultan en una polinomial. La no linealidad permite a la red aproximar funciones mas complejas.\n",
    "    * Son diferenciables, para poder calcular las gradientes a traves de ellas. Discontinuidades de punto como en `Hatdtanh` o `ReLU` son validas.\n",
    "* Sin esto, las redes caen a ser polinomiales complicadas o dificiles de entrenar.\n",
    "* Adicionalmente, las funciones:\n",
    "    * Tienen al menos un rango sensible, donde cambios no triviales en el input resultan en cambio no trivial correspondiente en el output\n",
    "    * Tienen al menos un rango no sensible (o saturado), donde cambios al input resultan en poco o ningun cambio en el output.\n",
    "* Por utlimo, las fuciones de activacion tienen al menos una de estas:\n",
    "    * Un limite inferior que se aproxima (o se encuentra) mientras el input tiende a negativo infinito.\n",
    "    * Un limite superior similar pero inverso para positivo infinito.\n",
    "* Dado lo que sabemos de como funciona back-propagation\n",
    "    * Sabemos que los errores se van a propagar hacia atras a traves de la activacion de manera mas efectiva cuando los inputs se encuentran dentro del rango de respuesta.\n",
    "    * Por otro lado, los errores no van a afectar a las neuornas para cuales el _input_ esta saturado debido a que la gradiente estara cercana a cero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En conclusion\n",
    "\n",
    "* En una red hecha de unidades lineales + activaciones, cuando recibe diferentes _inputs_:\n",
    "    * diferentes unidades van a responder en diferentes rangos para los mismos inputs\n",
    "    * los errores asociados a esos inputs van a afectar a las neuronas operancio en el rango sensible, dejando a las otras unidades mas o menos igual en el proceso de aprendizaje. \n",
    "* Juntar muchas operaciones lineales + unidades de activacion en paralelo y apilandolas una sobre otra nos provee un objeto matematico capaz de aproximar funciones complicadas. \n",
    "* Diferentes combinaciones de unidades van a responder a inputs en diferentes rangos\n",
    "    * Esos parametros son relativamente faciles de optimizar a traves de SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dibujo graficas computacionales separadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.7193],\n",
       "        [5.8628]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "linear_model = nn.Linear(1, 1)\n",
    "linear_model(val_t_un)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las subclases de `nn.Module` tienen un metodo `call` definido. Esto permite crear una instancia de `nn.Linear` y llamarla como si fuera una funcion.\n",
    "\n",
    "Llamar una instancia de `nn.Module` con un conjunto de argumetnos termina llamando un metodo llamado `forward` con esos mismos argumentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacion de `Module.call`\n",
    "\n",
    "(simplificado para claridad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, *input, **kwargs):\n",
    "    for hook in self._forward_pre_hooks.values():\n",
    "        hook(self, input)\n",
    "        \n",
    "    result = self.forward(*input, **kwargs)\n",
    "    \n",
    "    for hook in self._forward_hooks.values():\n",
    "        hook_result = hook(self, input, result)\n",
    "        # ...\n",
    "        \n",
    "    for hook in self._backward_hooks.values():\n",
    "        # ...\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De regreso al modelo lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.5043],\n",
       "        [4.2317]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "linear_model = nn.Linear(1, 1)\n",
    "linear_model(val_t_un)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Linear` acepta tres argumentos:\n",
    "* el numero de input features: size del input = 1\n",
    "* numero de output features: size del outpu = 1\n",
    "* si incluye un bias o no (por default es `True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.6012]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.1197], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7209], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(1)\n",
    "linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nuestro modelo toma un input y produce un output\n",
    "* `nn.Module` y sus subclases estan diseniados para hacer eso sobre multiples muestras al mismo tiempo\n",
    "* Para acomodar multiples muestras los modulos esperan que la dimension 0 del input sea el numero de muestras en un _batch_\n",
    "* Cualquier module en `nn` esta hecho para producir outputs para un _batch_ de multiples inputs al mismo tiempo.\n",
    "* B x Nin\n",
    "    * B es el tamanio del _batch_\n",
    "    * Nin el numero de input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7209],\n",
       "        [0.7209],\n",
       "        [0.7209],\n",
       "        [0.7209],\n",
       "        [0.7209],\n",
       "        [0.7209],\n",
       "        [0.7209],\n",
       "        [0.7209],\n",
       "        [0.7209],\n",
       "        [0.7209]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10, 1)\n",
    "linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para un dataset de imagenes:\n",
    "* BxCxHxW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0] # Temperatura en grados celsios\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4] # Unidades desconocidas\n",
    "t_c = torch.tensor(t_c).unsqueeze(1) # Agregamos una dimension para tener B x N_inputs\n",
    "t_u = torch.tensor(t_u).unsqueeze(1) # Agregamos una dimension para tener B x N_inputs\n",
    "\n",
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "params_old = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate_old = 1e-1\n",
    "optimizer_old = optim.Adam([params_old], lr=learning_rate_old)\n",
    "\n",
    "\n",
    "linear_model = nn.Linear(1, 1)\n",
    "optimizer = optim.SGD(\n",
    "    linear_model.parameters(), # reemplazamos [params] con este metodo \n",
    "    lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.8949]], requires_grad=True), Parameter containing:\n",
       " tensor([-0.0591], requires_grad=True)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(linear_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, n_epochs, optimizer, loss_fn, train_x, val_x, train_y, val_y):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_x) # ya no tenemos que pasar los params\n",
    "        train_loss = loss_fn(train_t_p, train_y)\n",
    "        \n",
    "        with torch.no_grad(): # todos los args requires_grad=False\n",
    "            val_t_p = model(val_x)\n",
    "            val_loss = loss_fn(val_t_p, val_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch == 1 or epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss}, Validation loss {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 80.94403839111328, Validation loss 128.55899047851562\n",
      "Epoch 1000, Training loss 3.501495838165283, Validation loss 4.327780723571777\n",
      "Epoch 2000, Training loss 2.88781476020813, Validation loss 3.3608384132385254\n",
      "Epoch 3000, Training loss 2.8758256435394287, Validation loss 3.248124599456787\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[5.2905]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-16.8822], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(1, 1)\n",
    "optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=3000,\n",
    "    optimizer=optimizer,\n",
    "    model=linear_model,\n",
    "    loss_fn=nn.MSELoss(), # Ya no estamos usando nuestra loss function hecha a mano\n",
    "    train_x = train_t_un,\n",
    "    val_x = val_t_un,\n",
    "    train_y = train_t_c,\n",
    "    val_y = val_t_c)\n",
    "\n",
    "print()\n",
    "print(linear_model.weight)\n",
    "print(linear_model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalmente un Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ultimo paso: reemplazar nuestro modelo lineal\n",
    "* No va a ser mejor\n",
    "* Lo unico que vamos a cambiar va a ser el modelo\n",
    "* Un simple NN:\n",
    "    * Una capa lineal\n",
    "    * Activacion\n",
    "    * \"hidden layers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=13, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=13, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model = nn.Sequential(\n",
    "                nn.Linear(1, 13), # El 13 es arbitrario\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(13, 1) # Este 13 debe hacer match con el primero\n",
    "            )\n",
    "\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El resultado final es un modelo que toma los inputs esperados por el primer modulo (_layer_)\n",
    "* Pasa los outputs intermedios al resto de los modulos\n",
    "* Produce un output retornado por el ultimo modulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([13, 1]), torch.Size([13]), torch.Size([1, 13]), torch.Size([1])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param.size() for param in seq_model.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Estos son los parametros que el optimizador va a recibir\n",
    "* Al llamar `backward()` todos los parametros se van a llenar con su `grad`\n",
    "* El optimizador va a actualizar el valor de `grad` durante `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([13, 1])\n",
      "0.bias torch.Size([13])\n",
      "2.weight torch.Size([1, 13])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in seq_model.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=13, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=13, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "named_seq_model = nn.Sequential(OrderedDict([\n",
    "        ('hidden_linear', nn.Linear(1, 8)),\n",
    "        ('hidden_activation', nn.Tanh()),\n",
    "        ('output_linear', nn.Linear(8, 1))\n",
    "]))\n",
    "\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_linear.weight torch.Size([8, 1])\n",
      "hidden_linear.bias torch.Size([8])\n",
      "output_linear.weight torch.Size([1, 8])\n",
      "output_linear.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in named_seq_model.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.1541], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_seq_model.output_linear.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Util para inspeccionar parametros o sus gradientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=5000,\n",
    "    optimizer=optimizer,\n",
    "    model=seq_model,\n",
    "    loss_fn=nn.MSELoss(), # Ya no estamos usando nuestra loss function hecha a mano\n",
    "    train_x = train_t_un,\n",
    "    val_x = val_t_un,\n",
    "    train_y = train_t_c,\n",
    "    val_y = val_t_c)\n",
    "\n",
    "print('output', seq_model(val_t_un))\n",
    "print('answer', val_t_c)\n",
    "print('hidden', seq_model.hidden_linear.weight.grad)\n",
    "\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start.record()\n",
    "z = x + y\n",
    "end.record()\n",
    "\n",
    "# Waits for everything to finish running\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(start.elapsed_time(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien podemos evaluar el modelo en toda la data y ver que tan diferente es de una linea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 3840x2880 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "t_range = torch.arange(20., 90.).unsqueeze(1)\n",
    "\n",
    "fig = plt.figure(dpi=600)\n",
    "plt.xlabel(\"Fahrenheit\")\n",
    "plt.ylabel(\"Celsius\")\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')\n",
    "plt.plot(t_range.numpy(), seq_model(0.1 * t_range).detach().numpy(), 'c-')\n",
    "plt.plot(t_u.numpy(), seq_model(0.1 * t_u).detach().numpy(), 'kx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclassing nn.Module\n",
    "\n",
    "* sublcassing `nn.Module` nos da mucha mas flexibilidad.\n",
    "* La interface especifica que como minimo debemos definir un metodo `forward` para la subclase\n",
    "    * `forward` toma el input al model y regresa el output\n",
    "* Si usamos las operaciones de `torch`, `autograd` se encarga de hacer el `backward` pass de forma automatica\n",
    "\n",
    "* Normalmente vamos a definir los submodulos que usamos en el metodo `forward` en el constructor\n",
    "    * Esto permite que sean llamados en `forward` y que puedan mantener sus parametros a durante la existencia de nuestro modulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SubclassModel(\n",
       "  (hidden_linear): Linear(in_features=1, out_features=13, bias=True)\n",
       "  (hidden_activation): Tanh()\n",
       "  (output_linear): Linear(in_features=13, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SubclassModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_linear = nn.Linear(1, 13)\n",
    "        self.hidden_activation = nn.Tanh()\n",
    "        self.output_linear = nn.Linear(13, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, input):\n",
    "        hidden_t = self.hidden_linear(input)\n",
    "        activated_t = self.hidden_activation(hidden_t)\n",
    "        #activated_t = self.hidden_activation(hidden_t) if random.random() > 0.5 else hidden_t\n",
    "        output_t = self.output_linear(activated_t)\n",
    "\n",
    "        return output_t\n",
    "\n",
    "    \n",
    "subclass_model = SubclassModel()\n",
    "subclass_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nos permite manipular los outputs de forma directa  y transformarlo en un tensor BxN\n",
    "* Dejamos la dimension de batch como -1 ya que no sabemos cuantos inputs van a venir por batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Asignar una instancia de `nn.Module` a un atributo en un `nn.Module` registra el modulo como un submodulo.\n",
    "* Permite a `Net` acceso a los `parameters` de sus submodulos sin necesidad de hacerlo manualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, [13, 13, 13, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in subclass_model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lo que paso**\n",
    "\n",
    "* `parameters()` investiga todos los submodulos asignados como atributos del constructor y llama `parameters` de forma recursiva.\n",
    "* Al accesar su atributo `grad`, el cual va a ser llenado por el `autograd`, el optimizador va a saber como cambiar los parametros para minimizar el _loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq\n",
      "0.weight              torch.Size([13, 1]) 13\n",
      "0.bias                torch.Size([13])    13\n",
      "2.weight              torch.Size([1, 13]) 13\n",
      "2.bias                torch.Size([1])     1\n",
      "\n",
      "named_seq\n",
      "hidden_linear.weight  torch.Size([8, 1])  8\n",
      "hidden_linear.bias    torch.Size([8])     8\n",
      "output_linear.weight  torch.Size([1, 8])  8\n",
      "output_linear.bias    torch.Size([1])     1\n",
      "\n",
      "subclass\n",
      "hidden_linear.weight  torch.Size([13, 1]) 13\n",
      "hidden_linear.bias    torch.Size([13])    13\n",
      "output_linear.weight  torch.Size([1, 13]) 13\n",
      "output_linear.bias    torch.Size([1])     1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for type_str, model in [('seq', seq_model), ('named_seq', named_seq_model), ('subclass', subclass_model)]:\n",
    "    print(type_str)\n",
    "    for name_str, param in model.named_parameters():\n",
    "        print(\"{:21} {:19} {}\".format(name_str, str(param.shape), param.numel()))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SubclassFunctionalModel(\n",
       "  (hidden_linear): Linear(in_features=1, out_features=14, bias=True)\n",
       "  (output_linear): Linear(in_features=14, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SubclassFunctionalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_linear = nn.Linear(1, 14)\n",
    "        self.output_linear = nn.Linear(14, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, input):\n",
    "        hidden_t = self.hidden_linear(input)\n",
    "        activated_t = torch.tanh(hidden_t)\n",
    "        output_t = self.output_linear(activated_t)\n",
    "\n",
    "        return output_t\n",
    "\n",
    "\n",
    "func_model = SubclassFunctionalModel()\n",
    "func_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Experimenten con el numero de neuronas en el modelo al igual que el learning rate.\n",
    "    * Que cambios resultan en un output mas lineal del modelo?\n",
    "    * Pueden hacer que el modelo haga un overfit obvio de la data?\n",
    "    \n",
    "* Cargen la [data de vinos blancos](https://archive.ics.uci.edu/ml/datasets/wine+quality) y creen un modelo con el numero apropiado de inputs\n",
    "    * Cuanto tarda en entrenar comparado al dataset que hemos estado usando?\n",
    "    * Pueden explicar que factores contribuyen a los tiempos de entrenamiento?\n",
    "    * Pueden hacer que el _loss_ disminuya?\n",
    "    * Intenten graficar la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 169.2747802734375, Validation loss 285.5258483886719\n",
      "Epoch 1000, Training loss 7.656148433685303, Validation loss 6.707472801208496\n",
      "Epoch 2000, Training loss 4.925140857696533, Validation loss 4.88743257522583\n",
      "Epoch 3000, Training loss 3.6442484855651855, Validation loss 3.8521175384521484\n",
      "Epoch 4000, Training loss 3.0405893325805664, Validation loss 3.232691526412964\n",
      "Epoch 5000, Training loss 2.745192766189575, Validation loss 2.8460631370544434\n",
      "Epoch 6000, Training loss 2.5898261070251465, Validation loss 2.595292806625366\n",
      "Epoch 7000, Training loss 2.4989142417907715, Validation loss 2.426912784576416\n",
      "Epoch 8000, Training loss 2.4386582374572754, Validation loss 2.3101367950439453\n",
      "Epoch 9000, Training loss 2.393911838531494, Validation loss 2.2265584468841553\n",
      "11.77637267112732\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "seq_model = nn.Sequential(\n",
    "                nn.Linear(1, 3000), \n",
    "                nn.Tanh(),\n",
    "                nn.Linear(3000, 1) # \n",
    "            )\n",
    "\n",
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-4)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=9000,\n",
    "    optimizer=optimizer,\n",
    "    model=seq_model,\n",
    "    loss_fn=nn.MSELoss(), # Se utiliza la función pytorch, no la generada manualmente\n",
    "    train_x = train_t_un,\n",
    "    val_x = val_t_un,\n",
    "    train_y = train_t_c,\n",
    "    val_y = val_t_c)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentando con el numero de neuronas en el modelo al igual que el learning rate.\n",
    "### Que cambios resultan en un output mas lineal del modelo?\n",
    "\n",
    "A un mayor número de repeticiones y neuronas la función de perdida reduce su tamaño, para lo cual se le atribuye mayor exactitud a un aumento del número de neuronas. Un numero de learning rate muy pequeño puede no implicar mejoras significativas, sin embargo el mejor resultado en la función loss se observa en el nivel 1e-4 debido a que cualquier denotación mayor no implicaba un aporte al modelo. \n",
    "\n",
    "\n",
    "\n",
    "### Pueden hacer que el modelo haga un overfit obvio de la data?\n",
    "\n",
    "Un overfit obvio se puede generar al crear demasiadas neuronas y al fijar el learning rate en el monto más alto permitido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whine_df = pd.read_csv(\"winequality-white.csv\", sep=\";\")\n",
    "whine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fixed acidity</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022697</td>\n",
       "      <td>0.289181</td>\n",
       "      <td>0.089021</td>\n",
       "      <td>0.023086</td>\n",
       "      <td>-0.049396</td>\n",
       "      <td>0.091070</td>\n",
       "      <td>0.265331</td>\n",
       "      <td>-0.425858</td>\n",
       "      <td>-0.017143</td>\n",
       "      <td>-0.120881</td>\n",
       "      <td>-0.113663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatile acidity</th>\n",
       "      <td>-0.022697</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.149472</td>\n",
       "      <td>0.064286</td>\n",
       "      <td>0.070512</td>\n",
       "      <td>-0.097012</td>\n",
       "      <td>0.089261</td>\n",
       "      <td>0.027114</td>\n",
       "      <td>-0.031915</td>\n",
       "      <td>-0.035728</td>\n",
       "      <td>0.067718</td>\n",
       "      <td>-0.194723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citric acid</th>\n",
       "      <td>0.289181</td>\n",
       "      <td>-0.149472</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.094212</td>\n",
       "      <td>0.114364</td>\n",
       "      <td>0.094077</td>\n",
       "      <td>0.121131</td>\n",
       "      <td>0.149503</td>\n",
       "      <td>-0.163748</td>\n",
       "      <td>0.062331</td>\n",
       "      <td>-0.075729</td>\n",
       "      <td>-0.009209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>residual sugar</th>\n",
       "      <td>0.089021</td>\n",
       "      <td>0.064286</td>\n",
       "      <td>0.094212</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.088685</td>\n",
       "      <td>0.299098</td>\n",
       "      <td>0.401439</td>\n",
       "      <td>0.838966</td>\n",
       "      <td>-0.194133</td>\n",
       "      <td>-0.026664</td>\n",
       "      <td>-0.450631</td>\n",
       "      <td>-0.097577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chlorides</th>\n",
       "      <td>0.023086</td>\n",
       "      <td>0.070512</td>\n",
       "      <td>0.114364</td>\n",
       "      <td>0.088685</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.101392</td>\n",
       "      <td>0.198910</td>\n",
       "      <td>0.257211</td>\n",
       "      <td>-0.090439</td>\n",
       "      <td>0.016763</td>\n",
       "      <td>-0.360189</td>\n",
       "      <td>-0.209934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <td>-0.049396</td>\n",
       "      <td>-0.097012</td>\n",
       "      <td>0.094077</td>\n",
       "      <td>0.299098</td>\n",
       "      <td>0.101392</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615501</td>\n",
       "      <td>0.294210</td>\n",
       "      <td>-0.000618</td>\n",
       "      <td>0.059217</td>\n",
       "      <td>-0.250104</td>\n",
       "      <td>0.008158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <td>0.091070</td>\n",
       "      <td>0.089261</td>\n",
       "      <td>0.121131</td>\n",
       "      <td>0.401439</td>\n",
       "      <td>0.198910</td>\n",
       "      <td>0.615501</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.529881</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.134562</td>\n",
       "      <td>-0.448892</td>\n",
       "      <td>-0.174737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>density</th>\n",
       "      <td>0.265331</td>\n",
       "      <td>0.027114</td>\n",
       "      <td>0.149503</td>\n",
       "      <td>0.838966</td>\n",
       "      <td>0.257211</td>\n",
       "      <td>0.294210</td>\n",
       "      <td>0.529881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.093591</td>\n",
       "      <td>0.074493</td>\n",
       "      <td>-0.780138</td>\n",
       "      <td>-0.307123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pH</th>\n",
       "      <td>-0.425858</td>\n",
       "      <td>-0.031915</td>\n",
       "      <td>-0.163748</td>\n",
       "      <td>-0.194133</td>\n",
       "      <td>-0.090439</td>\n",
       "      <td>-0.000618</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>-0.093591</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.155951</td>\n",
       "      <td>0.121432</td>\n",
       "      <td>0.099427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sulphates</th>\n",
       "      <td>-0.017143</td>\n",
       "      <td>-0.035728</td>\n",
       "      <td>0.062331</td>\n",
       "      <td>-0.026664</td>\n",
       "      <td>0.016763</td>\n",
       "      <td>0.059217</td>\n",
       "      <td>0.134562</td>\n",
       "      <td>0.074493</td>\n",
       "      <td>0.155951</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.017433</td>\n",
       "      <td>0.053678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alcohol</th>\n",
       "      <td>-0.120881</td>\n",
       "      <td>0.067718</td>\n",
       "      <td>-0.075729</td>\n",
       "      <td>-0.450631</td>\n",
       "      <td>-0.360189</td>\n",
       "      <td>-0.250104</td>\n",
       "      <td>-0.448892</td>\n",
       "      <td>-0.780138</td>\n",
       "      <td>0.121432</td>\n",
       "      <td>-0.017433</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.435575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>-0.113663</td>\n",
       "      <td>-0.194723</td>\n",
       "      <td>-0.009209</td>\n",
       "      <td>-0.097577</td>\n",
       "      <td>-0.209934</td>\n",
       "      <td>0.008158</td>\n",
       "      <td>-0.174737</td>\n",
       "      <td>-0.307123</td>\n",
       "      <td>0.099427</td>\n",
       "      <td>0.053678</td>\n",
       "      <td>0.435575</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      fixed acidity  volatile acidity  citric acid  \\\n",
       "fixed acidity              1.000000         -0.022697     0.289181   \n",
       "volatile acidity          -0.022697          1.000000    -0.149472   \n",
       "citric acid                0.289181         -0.149472     1.000000   \n",
       "residual sugar             0.089021          0.064286     0.094212   \n",
       "chlorides                  0.023086          0.070512     0.114364   \n",
       "free sulfur dioxide       -0.049396         -0.097012     0.094077   \n",
       "total sulfur dioxide       0.091070          0.089261     0.121131   \n",
       "density                    0.265331          0.027114     0.149503   \n",
       "pH                        -0.425858         -0.031915    -0.163748   \n",
       "sulphates                 -0.017143         -0.035728     0.062331   \n",
       "alcohol                   -0.120881          0.067718    -0.075729   \n",
       "quality                   -0.113663         -0.194723    -0.009209   \n",
       "\n",
       "                      residual sugar  chlorides  free sulfur dioxide  \\\n",
       "fixed acidity               0.089021   0.023086            -0.049396   \n",
       "volatile acidity            0.064286   0.070512            -0.097012   \n",
       "citric acid                 0.094212   0.114364             0.094077   \n",
       "residual sugar              1.000000   0.088685             0.299098   \n",
       "chlorides                   0.088685   1.000000             0.101392   \n",
       "free sulfur dioxide         0.299098   0.101392             1.000000   \n",
       "total sulfur dioxide        0.401439   0.198910             0.615501   \n",
       "density                     0.838966   0.257211             0.294210   \n",
       "pH                         -0.194133  -0.090439            -0.000618   \n",
       "sulphates                  -0.026664   0.016763             0.059217   \n",
       "alcohol                    -0.450631  -0.360189            -0.250104   \n",
       "quality                    -0.097577  -0.209934             0.008158   \n",
       "\n",
       "                      total sulfur dioxide   density        pH  sulphates  \\\n",
       "fixed acidity                     0.091070  0.265331 -0.425858  -0.017143   \n",
       "volatile acidity                  0.089261  0.027114 -0.031915  -0.035728   \n",
       "citric acid                       0.121131  0.149503 -0.163748   0.062331   \n",
       "residual sugar                    0.401439  0.838966 -0.194133  -0.026664   \n",
       "chlorides                         0.198910  0.257211 -0.090439   0.016763   \n",
       "free sulfur dioxide               0.615501  0.294210 -0.000618   0.059217   \n",
       "total sulfur dioxide              1.000000  0.529881  0.002321   0.134562   \n",
       "density                           0.529881  1.000000 -0.093591   0.074493   \n",
       "pH                                0.002321 -0.093591  1.000000   0.155951   \n",
       "sulphates                         0.134562  0.074493  0.155951   1.000000   \n",
       "alcohol                          -0.448892 -0.780138  0.121432  -0.017433   \n",
       "quality                          -0.174737 -0.307123  0.099427   0.053678   \n",
       "\n",
       "                       alcohol   quality  \n",
       "fixed acidity        -0.120881 -0.113663  \n",
       "volatile acidity      0.067718 -0.194723  \n",
       "citric acid          -0.075729 -0.009209  \n",
       "residual sugar       -0.450631 -0.097577  \n",
       "chlorides            -0.360189 -0.209934  \n",
       "free sulfur dioxide  -0.250104  0.008158  \n",
       "total sulfur dioxide -0.448892 -0.174737  \n",
       "density              -0.780138 -0.307123  \n",
       "pH                    0.121432  0.099427  \n",
       "sulphates            -0.017433  0.053678  \n",
       "alcohol               1.000000  0.435575  \n",
       "quality               0.435575  1.000000  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "corr = whine_df.corr()\n",
    " \n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = whine_df['alcohol']\n",
    "y = whine_df['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train = torch.tensor(X_train).unsqueeze(1)\n",
    "X_test = torch.tensor(X_test).unsqueeze(1)\n",
    "y_train = torch.tensor(y_train).unsqueeze(1)\n",
    "y_test = torch.tensor(y_test).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 35.2963981628418, Validation loss 35.6707649230957\n",
      "Epoch 1000, Training loss 0.741040050983429, Validation loss 0.7276855111122131\n",
      "Epoch 2000, Training loss 0.7219415307044983, Validation loss 0.7074940204620361\n",
      "Epoch 3000, Training loss 0.7064237594604492, Validation loss 0.6910156607627869\n",
      "Epoch 4000, Training loss 0.6938228011131287, Validation loss 0.6775680184364319\n",
      "Epoch 5000, Training loss 0.6836148500442505, Validation loss 0.6666131615638733\n",
      "422.0418703556061\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "seq_model = nn.Sequential(\n",
    "                nn.Linear(1, 1000), \n",
    "                nn.Tanh(),\n",
    "                nn.Linear(1000, 1) \n",
    "            )\n",
    "\n",
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-4)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=5000,\n",
    "    optimizer=optimizer,\n",
    "    model=seq_model,\n",
    "    loss_fn=nn.MSELoss(), \n",
    "    train_x = X_train.float(),\n",
    "    val_x = X_test.float(),\n",
    "    train_y = y_train.float(),\n",
    "    val_y = y_test.float())\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuanto tarda en entrenar comparado al dataset que hemos estado usando?\n",
    "\n",
    "#### Este dataset considerablemente toma más tiempo en entrenarse, se utilizo la función time.time para evaluar el tiempo elapsado y principalmente puedo atribuirlo al número de datos implicados en este set y la exactitud requerida. El primero modelo se entreno en 11 segundos mientras que el segundo demostro 422 segundos \n",
    "\n",
    "\n",
    "## Pueden explicar que factores contribuyen a los tiempos de entrenamiento?\n",
    "\n",
    "#### Los factores que explican el tiempo de entrenamiento son el numero de neuronas implicado en la red neuronal predominantemente, el tamaño del learning rate planteado y el número de repeticiones o epochs. Todas estas variables implican un aumento en la exactitud. \n",
    "\n",
    "## Pueden hacer que el loss disminuya?\n",
    "\n",
    "#### Al involucrar en el modelo las variables más correlacionadas como el alcohol para predecir la calidad del vino se reduce la función de perdida, al igual que al aumentar el learning rate al máximo disponible. \n",
    "\n",
    "## Intenten graficar la data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Juarez_Boris_Tarea4_neural_networks.ipynb to script\n",
      "[NbConvertApp] Writing 18339 bytes to Juarez_Boris_Tarea4_neural_networks.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Juarez_Boris_Tarea4_neural_networks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
