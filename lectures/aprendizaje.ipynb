{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Crear un algoritmo que _aprende_ de la data\n",
    "* Hacer un _fit_ a la data\n",
    "* El proceso: Una funcion con un numero de parametros desconocidos cuyos valores son estimados de la data\n",
    "* Un modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch esta diseniado para faciliatar la creacion de modelos para los cuales las derivadas del error con respecto a los parametros pueden ser expresadas de forma analitica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El aprendizaje es la estimacion de parametros\n",
    "\n",
    "* Data\n",
    "* Escoger un modelo\n",
    "* Estimar los parametros del modelo para tener buenas predicciones sobre data nueva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El problema\n",
    "\n",
    "Tenemos un termometro que no ensenia las unidades en las cuales muestra la temperatura.\n",
    "\n",
    "* data: lecturas del termometro y los valores correspondientes en una unidad conocida.\n",
    "* Escoger un modelo\n",
    "* Ajustar los parametros iterativamente hasta ue la medida del error sea lo suficientemente baja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0] # Temperatura en grados celsios\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4] # Unidades desconocidas\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escogiendo nuestro primer modelo\n",
    "\n",
    "t_c = w * t_u + b\n",
    "\n",
    "**Spoiler**: Sabemos que un modelo linear es el modelo correcto.\n",
    "\n",
    "* Tenemos un modelo con parametros desconocidos: $w$ y $b$\n",
    "* Tenemos que estimar esos parametros para que el error entre $\\hat{y}$ y $y$ sea lo mas pequenio posible\n",
    "* Todavia no hemos definido una medida de ese error.\n",
    "    * Esta medida, _loss function_, deberia ser alta si el error es alto e idealmente deberia ser lo mas bajo posible cuando haya un match perfecto.\n",
    "* Nuestro proceso de optimizacion deberia encontrar $w$ y $b$ para que el _loss function_ este en un minimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "* Una funcion que calcula un valor numerico que el proceso de optimizacion va a intentar minimizar.\n",
    "* El calculo del _loss_ normalmente involucra tomar la diferencia entre el output deseado para alguna muestra de entrenamiento y el verdadero output producido por el modelo cuando ve esos outputs.\n",
    "* En nuestro caso: `t_p - t_c`\n",
    "* Nuestra _loss_function_ deberia siempre ser positiva\n",
    "* Conceptualmente un _loss function_ es una forma de priorizar cuales errores de nuestro training sample arreglar, para que los ajustes a los parametros resulten en ajustes a los outputs para las muestras con mayor peso.\n",
    "\n",
    "\n",
    "Ejemplos:\n",
    "* $|t_p - t_c|$\n",
    "* $(t_p - t_c)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema a PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b\n",
    "\n",
    "\n",
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
       "        48.4000, 60.4000, 68.4000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones(1)\n",
    "b = torch.zeros(1)\n",
    "\n",
    "t_p = model(t_u, w, b)\n",
    "t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8846)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(t_p, t_c)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La gradiente\n",
    "\n",
    "**Como estimamos $w$ y $b$ para que el _loss_ llegue al minimo?**\n",
    "\n",
    "* _Gradient descent_\n",
    "    * Calcular la razon de cambio del _loss_ con respecto a cada parametro\n",
    "    * Aplicar un cambio a cada parametro en la direccion que reduzca el _loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "\n",
    "loss_rate_of_change_w = (loss_fn(model(t_u, w + delta, b), t_c) -\n",
    "                        loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conceptualmente:** \n",
    "* Un incremento unitario en $w$ va a provocar un cambio en el _loss_. \n",
    "    * Si el cambio es negativo, tenemos que incrementar $w$ para minimizar el _loss_\n",
    "    * Si el cambio es positivo, tenemos que reducir $w$\n",
    "    \n",
    "**Por cuanto debemos de incrementar o reducir el valor de $w$?**\n",
    "* Proporcional a la razon de cambio del _loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "w = w - learning_rate * loss_rate_of_change_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_rate_of_change_b = (loss_fn(model(t_u, w, b + delta), t_c) -\n",
    "                        loss_fn(model(t_u, w, b - delta), t_c) / (2.0 * delta))\n",
    "\n",
    "b = b - learning_rate * loss_rate_of_change_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Esto representa un paso en gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version analitica\n",
    "\n",
    "* Que pasa si el valor de delta fuera infinitesimamente pequenio?\n",
    "* La derivada del _loss_ con respecto a cada parametro.\n",
    "    * $\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial t_p} \\frac{\\partial t_p}{\\partial w}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c) ** 2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial x^2}{\\partial x} = 2x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_fn(t_p, t_c):\n",
    "    dsq_diffs = 2 * (t_p - t_c)\n",
    "    return dsq_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b\n",
    "\n",
    "\n",
    "def dmodel_dw(t_u, w, b):\n",
    "    return t_u\n",
    "\n",
    "\n",
    "def dmodel_db(t_u, w, b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funcion retornando la gradiente del _loss_ con respecto a $w$ y $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(t_u, t_c, t_p, w, b):\n",
    "    dloss_dw = dloss_fn(t_p, t_c) * dmodel_dw(t_u, w, b)\n",
    "    dloss_db = dloss_fn(t_p, t_c) * dmodel_db(t_u, w, b)\n",
    "    return torch.stack([dloss_dw.mean(), dloss_db.mean()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Ya tenemos todo para optimizar nuestros parametros:\n",
    "* Empezamos con un valor tentativo para cada paremetro\n",
    "* Actualizamos iterativamente\n",
    "* Paramos despues de un numero fijo de iteraciones\n",
    "* o hasta que $w$ y $b$ dejen de cambiar\n",
    "\n",
    "\n",
    "**Epoch**: una iteracion de entrenamiento durante la cual actualizamos los parametros para todo nuestro dataset de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, n_epochs, learning_rate, params, t_u, t_c, print_params=True):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "        \n",
    "        t_p = model(t_u, w, b) # Forward pass\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b) # Backward pass\n",
    "        \n",
    "        params = params - learning_rate * grad\n",
    "        \n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch}, Loss {loss}\")\n",
    "            if print_params:\n",
    "                print(f\"\\tParams: {params}\")\n",
    "                print(f\"\\tGrad: {grad}\")\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.8846435546875\n",
      "\tParams: tensor([-44.1730,  -0.8260])\n",
      "\tGrad: tensor([4517.2964,   82.6000])\n",
      "Epoch 10, Loss 9.090107547845813e+34\n",
      "\tParams: tensor([3.2144e+17, 5.6621e+15])\n",
      "\tGrad: tensor([-3.2700e+19, -5.7600e+17])\n",
      "Epoch 20, Loss inf\n",
      "\tParams: tensor([1.3457e+35, 2.3704e+33])\n",
      "\tGrad: tensor([-1.3690e+37, -2.4114e+35])\n",
      "Epoch 30, Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad: tensor([nan, nan])\n",
      "Epoch 40, Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad: tensor([nan, nan])\n",
      "Epoch 50, Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad: tensor([nan, nan])\n",
      "Epoch 60, Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad: tensor([nan, nan])\n",
      "Epoch 70, Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad: tensor([nan, nan])\n",
      "Epoch 80, Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad: tensor([nan, nan])\n",
      "Epoch 90, Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad: tensor([nan, nan])\n",
      "Epoch 100, Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad: tensor([nan, nan])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(model,\n",
    "              n_epochs = 100,\n",
    "              learning_rate= 1e-2,\n",
    "              params = torch.tensor([1.0, 0.0]),\n",
    "              t_u = t_u,\n",
    "              t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Que paso?\n",
    "\n",
    "* Como podemos limitar la magnitud de `learning_rate * grad`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.8846435546875\n",
      "\tParams: tensor([ 0.5483, -0.0083])\n",
      "\tGrad: tensor([4517.2964,   82.6000])\n",
      "Epoch 10, Loss 29.105241775512695\n",
      "\tParams: tensor([ 0.2324, -0.0166])\n",
      "\tGrad: tensor([1.4803, 3.0544])\n",
      "Epoch 20, Loss 29.095884323120117\n",
      "\tParams: tensor([ 0.2323, -0.0196])\n",
      "\tGrad: tensor([-0.0531,  3.0268])\n",
      "Epoch 30, Loss 29.08671760559082\n",
      "\tParams: tensor([ 0.2323, -0.0226])\n",
      "\tGrad: tensor([-0.0533,  3.0263])\n",
      "Epoch 40, Loss 29.077558517456055\n",
      "\tParams: tensor([ 0.2324, -0.0256])\n",
      "\tGrad: tensor([-0.0533,  3.0258])\n",
      "Epoch 50, Loss 29.068401336669922\n",
      "\tParams: tensor([ 0.2325, -0.0287])\n",
      "\tGrad: tensor([-0.0532,  3.0252])\n",
      "Epoch 60, Loss 29.05925178527832\n",
      "\tParams: tensor([ 0.2325, -0.0317])\n",
      "\tGrad: tensor([-0.0533,  3.0247])\n",
      "Epoch 70, Loss 29.050098419189453\n",
      "\tParams: tensor([ 0.2326, -0.0347])\n",
      "\tGrad: tensor([-0.0532,  3.0242])\n",
      "Epoch 80, Loss 29.04095458984375\n",
      "\tParams: tensor([ 0.2326, -0.0377])\n",
      "\tGrad: tensor([-0.0532,  3.0236])\n",
      "Epoch 90, Loss 29.03180503845215\n",
      "\tParams: tensor([ 0.2327, -0.0408])\n",
      "\tGrad: tensor([-0.0533,  3.0231])\n",
      "Epoch 100, Loss 29.022668838500977\n",
      "\tParams: tensor([ 0.2327, -0.0438])\n",
      "\tGrad: tensor([-0.0532,  3.0226])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2327, -0.0438])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(model,\n",
    "              n_epochs = 100,\n",
    "              learning_rate= 1e-4,\n",
    "              params = torch.tensor([1.0, 0.0]),\n",
    "              t_u = t_u,\n",
    "              t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejor\n",
    "\n",
    "El comportamiento se mantuvo estable\n",
    "\n",
    "Hay otro problema:\n",
    "* la gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 1, Loss 1763.8846435546875\n",
    "\tParams: tensor([ 0.5483, -0.0083])\n",
    "\tGrad: tensor([4517.2964,   82.6000])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.68881840193705"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4517.2964/82.6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.36434173583984\n",
      "\tParams: tensor([1.7761, 0.1064])\n",
      "\tGrad: tensor([-77.6140, -10.6400])\n",
      "Epoch 10, Loss 29.030487060546875\n",
      "\tParams: tensor([ 2.3232, -0.0710])\n",
      "\tGrad: tensor([-0.5355,  2.9295])\n",
      "Epoch 20, Loss 28.157800674438477\n",
      "\tParams: tensor([ 2.3746, -0.3615])\n",
      "\tGrad: tensor([-0.5093,  2.8832])\n",
      "Epoch 30, Loss 27.314294815063477\n",
      "\tParams: tensor([ 2.4251, -0.6471])\n",
      "\tGrad: tensor([-0.5007,  2.8346])\n",
      "Epoch 40, Loss 26.498987197875977\n",
      "\tParams: tensor([ 2.4747, -0.9280])\n",
      "\tGrad: tensor([-0.4923,  2.7868])\n",
      "Epoch 50, Loss 25.710935592651367\n",
      "\tParams: tensor([ 2.5235, -1.2040])\n",
      "\tGrad: tensor([-0.4840,  2.7398])\n",
      "Epoch 60, Loss 24.949237823486328\n",
      "\tParams: tensor([ 2.5714, -1.4755])\n",
      "\tGrad: tensor([-0.4758,  2.6936])\n",
      "Epoch 70, Loss 24.212995529174805\n",
      "\tParams: tensor([ 2.6186, -1.7423])\n",
      "\tGrad: tensor([-0.4678,  2.6482])\n",
      "Epoch 80, Loss 23.501379013061523\n",
      "\tParams: tensor([ 2.6649, -2.0047])\n",
      "\tGrad: tensor([-0.4599,  2.6035])\n",
      "Epoch 90, Loss 22.813547134399414\n",
      "\tParams: tensor([ 2.7105, -2.2626])\n",
      "\tGrad: tensor([-0.4522,  2.5597])\n",
      "Epoch 100, Loss 22.148710250854492\n",
      "\tParams: tensor([ 2.7553, -2.5162])\n",
      "\tGrad: tensor([-0.4445,  2.5165])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7553, -2.5162])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(model,\n",
    "              n_epochs = 100,\n",
    "              learning_rate= 1e-2,\n",
    "              params = torch.tensor([1.0, 0.0]),\n",
    "              t_u = t_un, # normalizado\n",
    "              t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.36434173583984\n",
      "Epoch 10, Loss 29.030487060546875\n",
      "Epoch 20, Loss 28.157800674438477\n",
      "Epoch 30, Loss 27.314294815063477\n",
      "Epoch 40, Loss 26.498987197875977\n",
      "Epoch 50, Loss 25.710935592651367\n",
      "Epoch 60, Loss 24.949237823486328\n",
      "Epoch 70, Loss 24.212995529174805\n",
      "Epoch 80, Loss 23.501379013061523\n",
      "Epoch 90, Loss 22.813547134399414\n",
      "Epoch 100, Loss 22.148710250854492\n",
      "Epoch 110, Loss 21.506103515625\n",
      "Epoch 120, Loss 20.88498306274414\n",
      "Epoch 130, Loss 20.284622192382812\n",
      "Epoch 140, Loss 19.70433807373047\n",
      "Epoch 150, Loss 19.14344596862793\n",
      "Epoch 160, Loss 18.6013126373291\n",
      "Epoch 170, Loss 18.077299118041992\n",
      "Epoch 180, Loss 17.57081413269043\n",
      "Epoch 190, Loss 17.081254959106445\n",
      "Epoch 200, Loss 16.608064651489258\n",
      "Epoch 210, Loss 16.150693893432617\n",
      "Epoch 220, Loss 15.708612442016602\n",
      "Epoch 230, Loss 15.281317710876465\n",
      "Epoch 240, Loss 14.868304252624512\n",
      "Epoch 250, Loss 14.469095230102539\n",
      "Epoch 260, Loss 14.083236694335938\n",
      "Epoch 270, Loss 13.710277557373047\n",
      "Epoch 280, Loss 13.349786758422852\n",
      "Epoch 290, Loss 13.001349449157715\n",
      "Epoch 300, Loss 12.664560317993164\n",
      "Epoch 310, Loss 12.339031219482422\n",
      "Epoch 320, Loss 12.024384498596191\n",
      "Epoch 330, Loss 11.720257759094238\n",
      "Epoch 340, Loss 11.426300048828125\n",
      "Epoch 350, Loss 11.142169952392578\n",
      "Epoch 360, Loss 10.867536544799805\n",
      "Epoch 370, Loss 10.602084159851074\n",
      "Epoch 380, Loss 10.34550952911377\n",
      "Epoch 390, Loss 10.097512245178223\n",
      "Epoch 400, Loss 9.857802391052246\n",
      "Epoch 410, Loss 9.626110076904297\n",
      "Epoch 420, Loss 9.402162551879883\n",
      "Epoch 430, Loss 9.185704231262207\n",
      "Epoch 440, Loss 8.976478576660156\n",
      "Epoch 450, Loss 8.774251937866211\n",
      "Epoch 460, Loss 8.578781127929688\n",
      "Epoch 470, Loss 8.389847755432129\n",
      "Epoch 480, Loss 8.207232475280762\n",
      "Epoch 490, Loss 8.030722618103027\n",
      "Epoch 500, Loss 7.8601155281066895\n",
      "Epoch 510, Loss 7.695211887359619\n",
      "Epoch 520, Loss 7.535818576812744\n",
      "Epoch 530, Loss 7.38175630569458\n",
      "Epoch 540, Loss 7.232845783233643\n",
      "Epoch 550, Loss 7.088911056518555\n",
      "Epoch 560, Loss 6.949786186218262\n",
      "Epoch 570, Loss 6.815318584442139\n",
      "Epoch 580, Loss 6.685344219207764\n",
      "Epoch 590, Loss 6.559711933135986\n",
      "Epoch 600, Loss 6.438284397125244\n",
      "Epoch 610, Loss 6.3209123611450195\n",
      "Epoch 620, Loss 6.207470893859863\n",
      "Epoch 630, Loss 6.097815036773682\n",
      "Epoch 640, Loss 5.991828918457031\n",
      "Epoch 650, Loss 5.8893842697143555\n",
      "Epoch 660, Loss 5.7903666496276855\n",
      "Epoch 670, Loss 5.694658279418945\n",
      "Epoch 680, Loss 5.602148056030273\n",
      "Epoch 690, Loss 5.512733459472656\n",
      "Epoch 700, Loss 5.426309585571289\n",
      "Epoch 710, Loss 5.342771053314209\n",
      "Epoch 720, Loss 5.262026309967041\n",
      "Epoch 730, Loss 5.183984279632568\n",
      "Epoch 740, Loss 5.108546733856201\n",
      "Epoch 750, Loss 5.0356364250183105\n",
      "Epoch 760, Loss 4.965158939361572\n",
      "Epoch 770, Loss 4.897039890289307\n",
      "Epoch 780, Loss 4.831196308135986\n",
      "Epoch 790, Loss 4.767558574676514\n",
      "Epoch 800, Loss 4.706046104431152\n",
      "Epoch 810, Loss 4.646591663360596\n",
      "Epoch 820, Loss 4.5891194343566895\n",
      "Epoch 830, Loss 4.53357458114624\n",
      "Epoch 840, Loss 4.479883670806885\n",
      "Epoch 850, Loss 4.427989482879639\n",
      "Epoch 860, Loss 4.377828121185303\n",
      "Epoch 870, Loss 4.329345226287842\n",
      "Epoch 880, Loss 4.282481670379639\n",
      "Epoch 890, Loss 4.237185001373291\n",
      "Epoch 900, Loss 4.1934051513671875\n",
      "Epoch 910, Loss 4.151086330413818\n",
      "Epoch 920, Loss 4.110184192657471\n",
      "Epoch 930, Loss 4.070649147033691\n",
      "Epoch 940, Loss 4.03243350982666\n",
      "Epoch 950, Loss 3.995497941970825\n",
      "Epoch 960, Loss 3.9597978591918945\n",
      "Epoch 970, Loss 3.9252915382385254\n",
      "Epoch 980, Loss 3.8919358253479004\n",
      "Epoch 990, Loss 3.859699010848999\n",
      "Epoch 1000, Loss 3.828537940979004\n",
      "Epoch 1010, Loss 3.7984206676483154\n",
      "Epoch 1020, Loss 3.7693099975585938\n",
      "Epoch 1030, Loss 3.7411692142486572\n",
      "Epoch 1040, Loss 3.7139718532562256\n",
      "Epoch 1050, Loss 3.687682867050171\n",
      "Epoch 1060, Loss 3.662273406982422\n",
      "Epoch 1070, Loss 3.6377112865448\n",
      "Epoch 1080, Loss 3.6139726638793945\n",
      "Epoch 1090, Loss 3.5910298824310303\n",
      "Epoch 1100, Loss 3.56884765625\n",
      "Epoch 1110, Loss 3.5474114418029785\n",
      "Epoch 1120, Loss 3.52669358253479\n",
      "Epoch 1130, Loss 3.5066652297973633\n",
      "Epoch 1140, Loss 3.487307548522949\n",
      "Epoch 1150, Loss 3.468597173690796\n",
      "Epoch 1160, Loss 3.450512170791626\n",
      "Epoch 1170, Loss 3.4330294132232666\n",
      "Epoch 1180, Loss 3.4161341190338135\n",
      "Epoch 1190, Loss 3.3998024463653564\n",
      "Epoch 1200, Loss 3.3840172290802\n",
      "Epoch 1210, Loss 3.3687593936920166\n",
      "Epoch 1220, Loss 3.354012966156006\n",
      "Epoch 1230, Loss 3.3397583961486816\n",
      "Epoch 1240, Loss 3.325979709625244\n",
      "Epoch 1250, Loss 3.3126630783081055\n",
      "Epoch 1260, Loss 3.299790859222412\n",
      "Epoch 1270, Loss 3.287346839904785\n",
      "Epoch 1280, Loss 3.2753219604492188\n",
      "Epoch 1290, Loss 3.263699531555176\n",
      "Epoch 1300, Loss 3.2524619102478027\n",
      "Epoch 1310, Loss 3.2416059970855713\n",
      "Epoch 1320, Loss 3.2311081886291504\n",
      "Epoch 1330, Loss 3.220961809158325\n",
      "Epoch 1340, Loss 3.2111566066741943\n",
      "Epoch 1350, Loss 3.2016775608062744\n",
      "Epoch 1360, Loss 3.192516565322876\n",
      "Epoch 1370, Loss 3.1836624145507812\n",
      "Epoch 1380, Loss 3.1751012802124023\n",
      "Epoch 1390, Loss 3.166827440261841\n",
      "Epoch 1400, Loss 3.1588308811187744\n",
      "Epoch 1410, Loss 3.1511011123657227\n",
      "Epoch 1420, Loss 3.143630266189575\n",
      "Epoch 1430, Loss 3.1364095211029053\n",
      "Epoch 1440, Loss 3.12943172454834\n",
      "Epoch 1450, Loss 3.122685432434082\n",
      "Epoch 1460, Loss 3.116163730621338\n",
      "Epoch 1470, Loss 3.1098597049713135\n",
      "Epoch 1480, Loss 3.1037697792053223\n",
      "Epoch 1490, Loss 3.0978829860687256\n",
      "Epoch 1500, Loss 3.092191219329834\n",
      "Epoch 1510, Loss 3.0866899490356445\n",
      "Epoch 1520, Loss 3.0813727378845215\n",
      "Epoch 1530, Loss 3.0762319564819336\n",
      "Epoch 1540, Loss 3.0712647438049316\n",
      "Epoch 1550, Loss 3.0664637088775635\n",
      "Epoch 1560, Loss 3.0618224143981934\n",
      "Epoch 1570, Loss 3.057337999343872\n",
      "Epoch 1580, Loss 3.0530006885528564\n",
      "Epoch 1590, Loss 3.048811197280884\n",
      "Epoch 1600, Loss 3.0447585582733154\n",
      "Epoch 1610, Loss 3.040844202041626\n",
      "Epoch 1620, Loss 3.0370588302612305\n",
      "Epoch 1630, Loss 3.0334017276763916\n",
      "Epoch 1640, Loss 3.0298662185668945\n",
      "Epoch 1650, Loss 3.026446580886841\n",
      "Epoch 1660, Loss 3.023145914077759\n",
      "Epoch 1670, Loss 3.019951820373535\n",
      "Epoch 1680, Loss 3.016867160797119\n",
      "Epoch 1690, Loss 3.0138838291168213\n",
      "Epoch 1700, Loss 3.0110013484954834\n",
      "Epoch 1710, Loss 3.0082144737243652\n",
      "Epoch 1720, Loss 3.0055201053619385\n",
      "Epoch 1730, Loss 3.002917528152466\n",
      "Epoch 1740, Loss 3.0004003047943115\n",
      "Epoch 1750, Loss 2.9979677200317383\n",
      "Epoch 1760, Loss 2.9956154823303223\n",
      "Epoch 1770, Loss 2.9933440685272217\n",
      "Epoch 1780, Loss 2.9911458492279053\n",
      "Epoch 1790, Loss 2.9890248775482178\n",
      "Epoch 1800, Loss 2.986973524093628\n",
      "Epoch 1810, Loss 2.9849894046783447\n",
      "Epoch 1820, Loss 2.9830727577209473\n",
      "Epoch 1830, Loss 2.9812192916870117\n",
      "Epoch 1840, Loss 2.979428291320801\n",
      "Epoch 1850, Loss 2.977696418762207\n",
      "Epoch 1860, Loss 2.9760231971740723\n",
      "Epoch 1870, Loss 2.9744057655334473\n",
      "Epoch 1880, Loss 2.972843885421753\n",
      "Epoch 1890, Loss 2.971331834793091\n",
      "Epoch 1900, Loss 2.9698705673217773\n",
      "Epoch 1910, Loss 2.9684598445892334\n",
      "Epoch 1920, Loss 2.967095136642456\n",
      "Epoch 1930, Loss 2.965776205062866\n",
      "Epoch 1940, Loss 2.9645001888275146\n",
      "Epoch 1950, Loss 2.963266134262085\n",
      "Epoch 1960, Loss 2.9620778560638428\n",
      "Epoch 1970, Loss 2.960926055908203\n",
      "Epoch 1980, Loss 2.959812879562378\n",
      "Epoch 1990, Loss 2.958738088607788\n",
      "Epoch 2000, Loss 2.9576973915100098\n",
      "Epoch 2010, Loss 2.9566941261291504\n",
      "Epoch 2020, Loss 2.955721855163574\n",
      "Epoch 2030, Loss 2.9547834396362305\n",
      "Epoch 2040, Loss 2.953875780105591\n",
      "Epoch 2050, Loss 2.953000068664551\n",
      "Epoch 2060, Loss 2.9521515369415283\n",
      "Epoch 2070, Loss 2.9513330459594727\n",
      "Epoch 2080, Loss 2.95054030418396\n",
      "Epoch 2090, Loss 2.9497756958007812\n",
      "Epoch 2100, Loss 2.94903564453125\n",
      "Epoch 2110, Loss 2.9483213424682617\n",
      "Epoch 2120, Loss 2.9476282596588135\n",
      "Epoch 2130, Loss 2.94696044921875\n",
      "Epoch 2140, Loss 2.9463140964508057\n",
      "Epoch 2150, Loss 2.945690393447876\n",
      "Epoch 2160, Loss 2.9450876712799072\n",
      "Epoch 2170, Loss 2.9445040225982666\n",
      "Epoch 2180, Loss 2.9439408779144287\n",
      "Epoch 2190, Loss 2.9433953762054443\n",
      "Epoch 2200, Loss 2.9428696632385254\n",
      "Epoch 2210, Loss 2.9423608779907227\n",
      "Epoch 2220, Loss 2.9418675899505615\n",
      "Epoch 2230, Loss 2.9413931369781494\n",
      "Epoch 2240, Loss 2.9409332275390625\n",
      "Epoch 2250, Loss 2.9404890537261963\n",
      "Epoch 2260, Loss 2.9400599002838135\n",
      "Epoch 2270, Loss 2.939645528793335\n",
      "Epoch 2280, Loss 2.9392433166503906\n",
      "Epoch 2290, Loss 2.938857078552246\n",
      "Epoch 2300, Loss 2.9384806156158447\n",
      "Epoch 2310, Loss 2.9381182193756104\n",
      "Epoch 2320, Loss 2.9377684593200684\n",
      "Epoch 2330, Loss 2.937429666519165\n",
      "Epoch 2340, Loss 2.9371042251586914\n",
      "Epoch 2350, Loss 2.9367880821228027\n",
      "Epoch 2360, Loss 2.9364817142486572\n",
      "Epoch 2370, Loss 2.936187505722046\n",
      "Epoch 2380, Loss 2.935900926589966\n",
      "Epoch 2390, Loss 2.9356260299682617\n",
      "Epoch 2400, Loss 2.935356855392456\n",
      "Epoch 2410, Loss 2.9350991249084473\n",
      "Epoch 2420, Loss 2.934852361679077\n",
      "Epoch 2430, Loss 2.9346094131469727\n",
      "Epoch 2440, Loss 2.9343771934509277\n",
      "Epoch 2450, Loss 2.9341509342193604\n",
      "Epoch 2460, Loss 2.9339349269866943\n",
      "Epoch 2470, Loss 2.933722734451294\n",
      "Epoch 2480, Loss 2.933521270751953\n",
      "Epoch 2490, Loss 2.9333252906799316\n",
      "Epoch 2500, Loss 2.933133840560913\n",
      "Epoch 2510, Loss 2.932952880859375\n",
      "Epoch 2520, Loss 2.9327738285064697\n",
      "Epoch 2530, Loss 2.932602882385254\n",
      "Epoch 2540, Loss 2.9324381351470947\n",
      "Epoch 2550, Loss 2.9322776794433594\n",
      "Epoch 2560, Loss 2.932121992111206\n",
      "Epoch 2570, Loss 2.9319722652435303\n",
      "Epoch 2580, Loss 2.931828260421753\n",
      "Epoch 2590, Loss 2.9316866397857666\n",
      "Epoch 2600, Loss 2.931553840637207\n",
      "Epoch 2610, Loss 2.931422472000122\n",
      "Epoch 2620, Loss 2.931295871734619\n",
      "Epoch 2630, Loss 2.931173086166382\n",
      "Epoch 2640, Loss 2.93105411529541\n",
      "Epoch 2650, Loss 2.930940866470337\n",
      "Epoch 2660, Loss 2.930832624435425\n",
      "Epoch 2670, Loss 2.9307243824005127\n",
      "Epoch 2680, Loss 2.930621862411499\n",
      "Epoch 2690, Loss 2.930523633956909\n",
      "Epoch 2700, Loss 2.9304258823394775\n",
      "Epoch 2710, Loss 2.9303343296051025\n",
      "Epoch 2720, Loss 2.930244207382202\n",
      "Epoch 2730, Loss 2.9301564693450928\n",
      "Epoch 2740, Loss 2.9300730228424072\n",
      "Epoch 2750, Loss 2.9299919605255127\n",
      "Epoch 2760, Loss 2.9299135208129883\n",
      "Epoch 2770, Loss 2.9298384189605713\n",
      "Epoch 2780, Loss 2.929764747619629\n",
      "Epoch 2790, Loss 2.929692029953003\n",
      "Epoch 2800, Loss 2.929626226425171\n",
      "Epoch 2810, Loss 2.929558753967285\n",
      "Epoch 2820, Loss 2.9294958114624023\n",
      "Epoch 2830, Loss 2.9294328689575195\n",
      "Epoch 2840, Loss 2.929373264312744\n",
      "Epoch 2850, Loss 2.9293158054351807\n",
      "Epoch 2860, Loss 2.929260492324829\n",
      "Epoch 2870, Loss 2.9292070865631104\n",
      "Epoch 2880, Loss 2.929154396057129\n",
      "Epoch 2890, Loss 2.9291038513183594\n",
      "Epoch 2900, Loss 2.9290544986724854\n",
      "Epoch 2910, Loss 2.929007053375244\n",
      "Epoch 2920, Loss 2.928961753845215\n",
      "Epoch 2930, Loss 2.9289183616638184\n",
      "Epoch 2940, Loss 2.928877353668213\n",
      "Epoch 2950, Loss 2.9288330078125\n",
      "Epoch 2960, Loss 2.9287948608398438\n",
      "Epoch 2970, Loss 2.928757667541504\n",
      "Epoch 2980, Loss 2.9287185668945312\n",
      "Epoch 2990, Loss 2.9286835193634033\n",
      "Epoch 3000, Loss 2.9286482334136963\n",
      "Epoch 3010, Loss 2.9286158084869385\n",
      "Epoch 3020, Loss 2.9285829067230225\n",
      "Epoch 3030, Loss 2.9285507202148438\n",
      "Epoch 3040, Loss 2.928520917892456\n",
      "Epoch 3050, Loss 2.92849063873291\n",
      "Epoch 3060, Loss 2.9284627437591553\n",
      "Epoch 3070, Loss 2.9284353256225586\n",
      "Epoch 3080, Loss 2.9284098148345947\n",
      "Epoch 3090, Loss 2.9283831119537354\n",
      "Epoch 3100, Loss 2.9283607006073\n",
      "Epoch 3110, Loss 2.9283368587493896\n",
      "Epoch 3120, Loss 2.9283125400543213\n",
      "Epoch 3130, Loss 2.9282915592193604\n",
      "Epoch 3140, Loss 2.9282681941986084\n",
      "Epoch 3150, Loss 2.928248405456543\n",
      "Epoch 3160, Loss 2.928227424621582\n",
      "Epoch 3170, Loss 2.9282095432281494\n",
      "Epoch 3180, Loss 2.928189992904663\n",
      "Epoch 3190, Loss 2.9281702041625977\n",
      "Epoch 3200, Loss 2.9281539916992188\n",
      "Epoch 3210, Loss 2.9281365871429443\n",
      "Epoch 3220, Loss 2.9281210899353027\n",
      "Epoch 3230, Loss 2.9281046390533447\n",
      "Epoch 3240, Loss 2.9280898571014404\n",
      "Epoch 3250, Loss 2.928074836730957\n",
      "Epoch 3260, Loss 2.92806077003479\n",
      "Epoch 3270, Loss 2.9280457496643066\n",
      "Epoch 3280, Loss 2.9280340671539307\n",
      "Epoch 3290, Loss 2.928020715713501\n",
      "Epoch 3300, Loss 2.928006172180176\n",
      "Epoch 3310, Loss 2.927995443344116\n",
      "Epoch 3320, Loss 2.927983283996582\n",
      "Epoch 3330, Loss 2.927973985671997\n",
      "Epoch 3340, Loss 2.927962303161621\n",
      "Epoch 3350, Loss 2.927950382232666\n",
      "Epoch 3360, Loss 2.927940607070923\n",
      "Epoch 3370, Loss 2.9279298782348633\n",
      "Epoch 3380, Loss 2.927922248840332\n",
      "Epoch 3390, Loss 2.9279115200042725\n",
      "Epoch 3400, Loss 2.927903890609741\n",
      "Epoch 3410, Loss 2.927896022796631\n",
      "Epoch 3420, Loss 2.9278855323791504\n",
      "Epoch 3430, Loss 2.9278790950775146\n",
      "Epoch 3440, Loss 2.927870035171509\n",
      "Epoch 3450, Loss 2.927863359451294\n",
      "Epoch 3460, Loss 2.927856683731079\n",
      "Epoch 3470, Loss 2.9278488159179688\n",
      "Epoch 3480, Loss 2.9278414249420166\n",
      "Epoch 3490, Loss 2.9278366565704346\n",
      "Epoch 3500, Loss 2.927830696105957\n",
      "Epoch 3510, Loss 2.9278228282928467\n",
      "Epoch 3520, Loss 2.927816867828369\n",
      "Epoch 3530, Loss 2.927811622619629\n",
      "Epoch 3540, Loss 2.927805185317993\n",
      "Epoch 3550, Loss 2.9277994632720947\n",
      "Epoch 3560, Loss 2.92779541015625\n",
      "Epoch 3570, Loss 2.927788734436035\n",
      "Epoch 3580, Loss 2.9277868270874023\n",
      "Epoch 3590, Loss 2.9277803897857666\n",
      "Epoch 3600, Loss 2.927776336669922\n",
      "Epoch 3610, Loss 2.9277732372283936\n",
      "Epoch 3620, Loss 2.9277684688568115\n",
      "Epoch 3630, Loss 2.927764892578125\n",
      "Epoch 3640, Loss 2.9277610778808594\n",
      "Epoch 3650, Loss 2.9277572631835938\n",
      "Epoch 3660, Loss 2.9277520179748535\n",
      "Epoch 3670, Loss 2.927748918533325\n",
      "Epoch 3680, Loss 2.9277467727661133\n",
      "Epoch 3690, Loss 2.9277429580688477\n",
      "Epoch 3700, Loss 2.9277384281158447\n",
      "Epoch 3710, Loss 2.927734375\n",
      "Epoch 3720, Loss 2.9277329444885254\n",
      "Epoch 3730, Loss 2.9277291297912598\n",
      "Epoch 3740, Loss 2.927727460861206\n",
      "Epoch 3750, Loss 2.927724599838257\n",
      "Epoch 3760, Loss 2.927720785140991\n",
      "Epoch 3770, Loss 2.9277191162109375\n",
      "Epoch 3780, Loss 2.9277162551879883\n",
      "Epoch 3790, Loss 2.9277150630950928\n",
      "Epoch 3800, Loss 2.9277124404907227\n",
      "Epoch 3810, Loss 2.92771053314209\n",
      "Epoch 3820, Loss 2.9277069568634033\n",
      "Epoch 3830, Loss 2.927706003189087\n",
      "Epoch 3840, Loss 2.927703619003296\n",
      "Epoch 3850, Loss 2.927700996398926\n",
      "Epoch 3860, Loss 2.9276998043060303\n",
      "Epoch 3870, Loss 2.92769718170166\n",
      "Epoch 3880, Loss 2.927696466445923\n",
      "Epoch 3890, Loss 2.927694082260132\n",
      "Epoch 3900, Loss 2.9276926517486572\n",
      "Epoch 3910, Loss 2.927691698074341\n",
      "Epoch 3920, Loss 2.927689552307129\n",
      "Epoch 3930, Loss 2.927687883377075\n",
      "Epoch 3940, Loss 2.9276864528656006\n",
      "Epoch 3950, Loss 2.9276864528656006\n",
      "Epoch 3960, Loss 2.9276840686798096\n",
      "Epoch 3970, Loss 2.9276819229125977\n",
      "Epoch 3980, Loss 2.9276812076568604\n",
      "Epoch 3990, Loss 2.9276809692382812\n",
      "Epoch 4000, Loss 2.927680253982544\n",
      "Epoch 4010, Loss 2.927678108215332\n",
      "Epoch 4020, Loss 2.927677869796753\n",
      "Epoch 4030, Loss 2.9276742935180664\n",
      "Epoch 4040, Loss 2.927675724029541\n",
      "Epoch 4050, Loss 2.9276726245880127\n",
      "Epoch 4060, Loss 2.9276747703552246\n",
      "Epoch 4070, Loss 2.9276719093322754\n",
      "Epoch 4080, Loss 2.9276716709136963\n",
      "Epoch 4090, Loss 2.927670478820801\n",
      "Epoch 4100, Loss 2.927671194076538\n",
      "Epoch 4110, Loss 2.9276680946350098\n",
      "Epoch 4120, Loss 2.9276697635650635\n",
      "Epoch 4130, Loss 2.927666664123535\n",
      "Epoch 4140, Loss 2.927665948867798\n",
      "Epoch 4150, Loss 2.9276654720306396\n",
      "Epoch 4160, Loss 2.9276657104492188\n",
      "Epoch 4170, Loss 2.927664279937744\n",
      "Epoch 4180, Loss 2.9276633262634277\n",
      "Epoch 4190, Loss 2.9276633262634277\n",
      "Epoch 4200, Loss 2.927663564682007\n",
      "Epoch 4210, Loss 2.927664041519165\n",
      "Epoch 4220, Loss 2.9276628494262695\n",
      "Epoch 4230, Loss 2.9276602268218994\n",
      "Epoch 4240, Loss 2.9276604652404785\n",
      "Epoch 4250, Loss 2.9276604652404785\n",
      "Epoch 4260, Loss 2.9276583194732666\n",
      "Epoch 4270, Loss 2.927659511566162\n",
      "Epoch 4280, Loss 2.9276578426361084\n",
      "Epoch 4290, Loss 2.92765736579895\n",
      "Epoch 4300, Loss 2.927659034729004\n",
      "Epoch 4310, Loss 2.927657127380371\n",
      "Epoch 4320, Loss 2.927656650543213\n",
      "Epoch 4330, Loss 2.927656650543213\n",
      "Epoch 4340, Loss 2.92765736579895\n",
      "Epoch 4350, Loss 2.9276556968688965\n",
      "Epoch 4360, Loss 2.9276559352874756\n",
      "Epoch 4370, Loss 2.927656412124634\n",
      "Epoch 4380, Loss 2.9276540279388428\n",
      "Epoch 4390, Loss 2.9276540279388428\n",
      "Epoch 4400, Loss 2.9276540279388428\n",
      "Epoch 4410, Loss 2.927654981613159\n",
      "Epoch 4420, Loss 2.927654504776001\n",
      "Epoch 4430, Loss 2.9276540279388428\n",
      "Epoch 4440, Loss 2.927654266357422\n",
      "Epoch 4450, Loss 2.9276540279388428\n",
      "Epoch 4460, Loss 2.9276528358459473\n",
      "Epoch 4470, Loss 2.927652597427368\n",
      "Epoch 4480, Loss 2.927652597427368\n",
      "Epoch 4490, Loss 2.927651882171631\n",
      "Epoch 4500, Loss 2.9276506900787354\n",
      "Epoch 4510, Loss 2.927651882171631\n",
      "Epoch 4520, Loss 2.9276514053344727\n",
      "Epoch 4530, Loss 2.9276511669158936\n",
      "Epoch 4540, Loss 2.9276511669158936\n",
      "Epoch 4550, Loss 2.927651882171631\n",
      "Epoch 4560, Loss 2.9276504516601562\n",
      "Epoch 4570, Loss 2.92765212059021\n",
      "Epoch 4580, Loss 2.927651882171631\n",
      "Epoch 4590, Loss 2.9276504516601562\n",
      "Epoch 4600, Loss 2.9276504516601562\n",
      "Epoch 4610, Loss 2.9276490211486816\n",
      "Epoch 4620, Loss 2.92764949798584\n",
      "Epoch 4630, Loss 2.9276506900787354\n",
      "Epoch 4640, Loss 2.92764949798584\n",
      "Epoch 4650, Loss 2.927650213241577\n",
      "Epoch 4660, Loss 2.9276487827301025\n",
      "Epoch 4670, Loss 2.927649736404419\n",
      "Epoch 4680, Loss 2.9276483058929443\n",
      "Epoch 4690, Loss 2.9276490211486816\n",
      "Epoch 4700, Loss 2.927649736404419\n",
      "Epoch 4710, Loss 2.9276490211486816\n",
      "Epoch 4720, Loss 2.9276487827301025\n",
      "Epoch 4730, Loss 2.9276490211486816\n",
      "Epoch 4740, Loss 2.9276487827301025\n",
      "Epoch 4750, Loss 2.927649736404419\n",
      "Epoch 4760, Loss 2.92764949798584\n",
      "Epoch 4770, Loss 2.927647590637207\n",
      "Epoch 4780, Loss 2.92764949798584\n",
      "Epoch 4790, Loss 2.927647590637207\n",
      "Epoch 4800, Loss 2.9276483058929443\n",
      "Epoch 4810, Loss 2.9276480674743652\n",
      "Epoch 4820, Loss 2.9276480674743652\n",
      "Epoch 4830, Loss 2.9276468753814697\n",
      "Epoch 4840, Loss 2.9276468753814697\n",
      "Epoch 4850, Loss 2.927647352218628\n",
      "Epoch 4860, Loss 2.9276487827301025\n",
      "Epoch 4870, Loss 2.9276483058929443\n",
      "Epoch 4880, Loss 2.9276483058929443\n",
      "Epoch 4890, Loss 2.9276480674743652\n",
      "Epoch 4900, Loss 2.9276480674743652\n",
      "Epoch 4910, Loss 2.927647590637207\n",
      "Epoch 4920, Loss 2.927647590637207\n",
      "Epoch 4930, Loss 2.927647590637207\n",
      "Epoch 4940, Loss 2.9276463985443115\n",
      "Epoch 4950, Loss 2.9276483058929443\n",
      "Epoch 4960, Loss 2.9276468753814697\n",
      "Epoch 4970, Loss 2.9276480674743652\n",
      "Epoch 4980, Loss 2.9276466369628906\n",
      "Epoch 4990, Loss 2.9276483058929443\n",
      "Epoch 5000, Loss 2.927647590637207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = training_loop(model,\n",
    "                       n_epochs = 5000,\n",
    "                       learning_rate= 1e-2,\n",
    "                       params = torch.tensor([1.0, 0.0]),\n",
    "                       t_u = t_un,\n",
    "                       t_c = t_c,\n",
    "                       print_params=False)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El _loss_ disminuyo mientras fuimos actualizando los parametros en la direccion de _gradient descent_\n",
    "* No llego a cero\n",
    "    * Porque?\n",
    "    \n",
    "* Sin embargo, los valores de $w$ y $b$ quedaron bastante cerca a los valores necesarios para convertir a Celsius a Fahrenheit.\n",
    "* Los valores exactos son:\n",
    "    * $w=5.5556$\n",
    "    * $b=-17.7778$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e8848d0c73db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mt_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_un\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "t_p = model(t_un, *params)\n",
    "\n",
    "fig = plt.figure(dpi=600)\n",
    "plt.xlabel(\"Fahrenheit\")\n",
    "plt.ylabel(\"Celsius\")\n",
    "plt.plot(t_u.numpy(), t_p.detach().numpy())\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Autograd\n",
    "\n",
    "* Back-propagation\n",
    "    * calcular la gradiente de una composicion de funciones - el modelo y el _loss_ - con respecto a sus parametros - $w$ y $b$ - propagando derivadas hacia atras usando la regla de la cadena.\n",
    "* Todas las funciones que deben ser diferenciables.\n",
    "    * Calcular la gradiente: la razon de cambio del _loss_ con respecto a los parametros\n",
    "\n",
    "**Que pasa cuando tenemos un modelo con millones de parametros?**\n",
    "* Funcion diferenciable\n",
    "* calcular la gradiente\n",
    "* composicion de varias funciones lineales y no lineales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b\n",
    "\n",
    "\n",
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`requires_grad=True`: Le estamos diciendo a PyTorch que mantenga un registro de todos los tensores resultantes de operaciones sobre `params`.\n",
    "\n",
    "Cualquier tensor que tenga `params` como ancestro va a tener acceso a la cadena de funciones que se llamaron para llegar de `params` a ese tensor.\n",
    "\n",
    "En caso que estas funciones sean diferenciables (la mayoria de operaciones en PyTorch lo son), el valor de la derivada va a ser automaticamente llenado como el atributo `grad` del tensor `params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4517.2969,   82.6000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(model(t_u, *params), t_c)\n",
    "loss.backward()\n",
    "\n",
    "params.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dibujo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Podemos tener $n$ tensores con `requires_grad=True`\n",
    "* y cualquier composicion de funciones\n",
    "* PyTorch calcularia las derivadas del _loss_ a traves de la cadena de esas funciones (grafica computacional)\n",
    "* **Acumularia** los valores en el atributo `grad` de esos tensores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning\n",
    "\n",
    "Llamar `backward` hace que las derivadas se **acumulen**. Tenemos que regresar la gradiente a cero explicitamente despues de usarla para actualizar parametros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.grad is not None:\n",
    "    params.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "            \n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        # grad = grad_fn(t_u, t_c, t_p, w, b)\n",
    "        loss.backward()\n",
    "        \n",
    "        params = (params - learning_rate * params.grad).detach().requires_grad_()\n",
    "        \n",
    "        if epoch % 200 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss {loss}\")\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`params = (params - learning_rate * params.grad).detach().requires_grad_()`\n",
    "\n",
    "Pensemos en la grafica computacional:\n",
    "* `p1 = (p0 * lr * p0.grad)`\n",
    "    * `p0` son los _weights_ del modelo\n",
    "    * `p0.grad` se calcula de una combinacion de `p0` y nuestra data a traves del _loss function_\n",
    "* `p2 = (p1 * lr * p1.grad)`\n",
    "* La grafica computacional de `p1` regresa a `p0` esto representa un problema:\n",
    "    * Tenemos que mantener `p0` en memoria hasta que termine el training\n",
    "    * Confunde donde tenemos que asignar el error a traves de back-prop\n",
    "* En vez, despegamos el nuevo tensor de `params` de la grafica computacional usando `detach()`\n",
    "* De esta forma `params` pierde la memoria de las operaciones que lo generaron.\n",
    "* Reanudamos el tracking llamando `requires_grad_()`\n",
    "* Ahora podemos deshacernos de la memoria mantenida por las versiones viejas de `params` y solo hacemos back-prop con los _weights_ actuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200, Loss 16.608064651489258\n",
      "Epoch 400, Loss 9.857802391052246\n",
      "Epoch 600, Loss 6.438284397125244\n",
      "Epoch 800, Loss 4.706046104431152\n",
      "Epoch 1000, Loss 3.828537940979004\n",
      "Epoch 1200, Loss 3.3840172290802\n",
      "Epoch 1400, Loss 3.1588308811187744\n",
      "Epoch 1600, Loss 3.0447585582733154\n",
      "Epoch 1800, Loss 2.986973524093628\n",
      "Epoch 2000, Loss 2.9576973915100098\n",
      "Epoch 2200, Loss 2.9428696632385254\n",
      "Epoch 2400, Loss 2.935356855392456\n",
      "Epoch 2600, Loss 2.931553840637207\n",
      "Epoch 2800, Loss 2.929626226425171\n",
      "Epoch 3000, Loss 2.9286482334136963\n",
      "Epoch 3200, Loss 2.9281539916992188\n",
      "Epoch 3400, Loss 2.927903890609741\n",
      "Epoch 3600, Loss 2.9277760982513428\n",
      "Epoch 3800, Loss 2.9277124404907227\n",
      "Epoch 4000, Loss 2.9276793003082275\n",
      "Epoch 4200, Loss 2.9276628494262695\n",
      "Epoch 4400, Loss 2.9276552200317383\n",
      "Epoch 4600, Loss 2.9276490211486816\n",
      "Epoch 4800, Loss 2.927647590637207\n",
      "Epoch 5000, Loss 2.9276468753814697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(model=model,\n",
    "              n_epochs=5000,\n",
    "              learning_rate=1e-2,\n",
    "              params=torch.tensor([1.0, 0.0], requires_grad=True), # CLAVE\n",
    "              t_u = t_un, # Seguimos usando la version normalizada\n",
    "              t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya no tenemos que calcular derivadas a mano :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizadores\n",
    "\n",
    "* Hemos estado utilizando gradient descent normal para optimizacion.\n",
    "* PyTorch abstrae la estrategia de optimizacion\n",
    "* El modulo `torch` tiene un submodulo `optim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'Optimizer',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'lr_scheduler']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo optmizador:\n",
    "* toma una lista de parametros (tensores de PyTorch, usualmente con `requires_grad=True`) y mantiene una referencia a ellos.\n",
    "* luego de que el _loss_ sea calculado con los inputs\n",
    "* una llamada a `.backward()` provoca que se llene `.grad` en los parametros\n",
    "* en ese punto, el optimizador puede accesar `.grad` para actualizar los parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SGD = Stochastic Gradient Descent\n",
    "* Exactamente lo mimsmo que hemos estado haciendo a mano (siempre y cuando el argumento `momentum` este en su valor default de 0.0)\n",
    "* El termino _stochastic_ viene del hecho que la gradiente normalmente se obitene promediando sobre un subset aleatorio de todos los inputs, llamado _minibatch_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El valor de `params` se actualizo cuando llamamos `step`\n",
    "* El optimizador utilizo los valores en `params.grad` y actualizo los parametros restando `(lr * grad)` de ellos.\n",
    "\n",
    "**Que se nos olvido hacer arriba?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7761, 0.1064], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_p = model(t_un, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 200 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss {loss}\")\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200, Loss 16.608064651489258\n",
      "Epoch 400, Loss 9.857802391052246\n",
      "Epoch 600, Loss 6.438284397125244\n",
      "Epoch 800, Loss 4.706046104431152\n",
      "Epoch 1000, Loss 3.828537940979004\n",
      "Epoch 1200, Loss 3.3840172290802\n",
      "Epoch 1400, Loss 3.1588308811187744\n",
      "Epoch 1600, Loss 3.0447585582733154\n",
      "Epoch 1800, Loss 2.986973524093628\n",
      "Epoch 2000, Loss 2.9576973915100098\n",
      "Epoch 2200, Loss 2.9428696632385254\n",
      "Epoch 2400, Loss 2.935356855392456\n",
      "Epoch 2600, Loss 2.931553840637207\n",
      "Epoch 2800, Loss 2.929626226425171\n",
      "Epoch 3000, Loss 2.9286482334136963\n",
      "Epoch 3200, Loss 2.9281539916992188\n",
      "Epoch 3400, Loss 2.927903890609741\n",
      "Epoch 3600, Loss 2.9277760982513428\n",
      "Epoch 3800, Loss 2.9277124404907227\n",
      "Epoch 4000, Loss 2.9276793003082275\n",
      "Epoch 4200, Loss 2.9276628494262695\n",
      "Epoch 4400, Loss 2.9276552200317383\n",
      "Epoch 4600, Loss 2.9276490211486816\n",
      "Epoch 4800, Loss 2.927647590637207\n",
      "Epoch 5000, Loss 2.9276468753814697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "training_loop(model,\n",
    "              n_epochs=5000,\n",
    "              optimizer=optimizer,\n",
    "             params = params, # Es importante que ambos `params` sean el mismo objeto\n",
    "             t_u = t_un,\n",
    "             t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200, Loss 19.08226203918457\n",
      "Epoch 400, Loss 10.508145332336426\n",
      "Epoch 600, Loss 5.641381740570068\n",
      "Epoch 800, Loss 3.6775739192962646\n",
      "Epoch 1000, Loss 3.0866997241973877\n",
      "Epoch 1200, Loss 2.9531853199005127\n",
      "Epoch 1400, Loss 2.9306914806365967\n",
      "Epoch 1600, Loss 2.927907705307007\n",
      "Epoch 1800, Loss 2.927661657333374\n",
      "Epoch 2000, Loss 2.9276459217071533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0.5367, -17.3021], requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-1\n",
    "optimizer = optim.Adam([params], lr=learning_rate) # Nuevo optimizador\n",
    "\n",
    "training_loop(model,\n",
    "              n_epochs=2000,\n",
    "              optimizer=optimizer,\n",
    "              params = params,\n",
    "              t_u = t_u, # Regresamos a usar el t_u original como input\n",
    "              t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En resumen\n",
    "\n",
    "* Back-propagation para estimar la gradiente\n",
    "* autograd\n",
    "* optimizar los pesos del modelo usando la SGD u otros optimizadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Redefinan el model a `w2 * t_u ** 2 + w1 * t_u + b`\n",
    "    * Que partes del training loop necesitaron cambiar para acomodar el nuevo modelo?\n",
    "    * Que partes se mantuvieron iguales?\n",
    "    * El _loss_ resultante es mas alto o bajo despues de entrenamiento?\n",
    "    * El resultado es mejor o peor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(model,\n",
    "              n_epochs=2000,\n",
    "              optimizer=optimizer,\n",
    "              params = params,\n",
    "              t_u = t_u, # Regresamos a usar el t_u original como input\n",
    "              t_c = t_c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
