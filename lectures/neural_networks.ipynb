{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El uso de funciones de activacion no lineares como la diferencia clave entre modelos lineales\n",
    "* Los diferentes tipos de funciones de activacion\n",
    "* El modulo `nn` de PyTorch que contiene los bloques para construir NNs\n",
    "* Resolver un problema simple de un _fit_ lineal con una NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronas artificiales\n",
    "\n",
    "* Neural networks: entidades matematicas capaces de representar funciones complicadas a traves de una composicion de funciones mas simples.\n",
    "* Originalmente inspiradas por la forma en la que funciona nuestro cerebro.\n",
    "* El bloque de construccion basico es una neurona:\n",
    "    * Esencialmente una transformacion linear del input (e.g. multiplicacion del input por un numero, el _weight_, y la suma de una constante, el _bias_.\n",
    "    * Seguido por la aplicacion de una funcion no lineal (referida como la funcion de activacion)\n",
    "    * $o = f(w x + b)$\n",
    "    * x es nuestro input, w el _weight_ y b el _bias_. $f$ es la funcion de activacion.\n",
    "    * x puede ser un escalar o un vector de valores, w puede ser un escalar o una matriz, mientras que b es un escalar o un vector.\n",
    "* La expresion $o = f(w x + b)$ es una capa de neuronas, ya que representa varias neuronas a traves de los _weights_ y _bias_ multidimensionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_1 = f(w_0 x_0 + b_0)$\n",
    "\n",
    "$x_2 = f(w_1 x_1 + b_1)$\n",
    "\n",
    "$...$\n",
    "\n",
    "$y = f(w_n x_n + b_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **dibujos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de activacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nuestro modelo anterior ya tenia una operacion lineal. Eso era el modelo entero.\n",
    "* El rol de la funcion de activacion es concentrar los _outputs_ de la operacion lineal precedente a un rango dado.\n",
    "* Si queremos asignar un _score_ al output del modelo necesitamos limitar el rango de numeros posibles para ese _score_\n",
    "    * `float32`\n",
    "    * $\\sum wx + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Que opciones tenemos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Una opcion seria ponerle un limite a los valores del _output_.\n",
    "    * Cualquier cosa debajo de cero seria cero\n",
    "    * cualquier cosa arriba de 10 seria 10\n",
    "    * `torch.nn.Hardtanh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9757431300314515"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "math.tanh(-2.2) # camion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09966799462495582"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.tanh(0.1) # oso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866142981514303"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.tanh(2.5) # perro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Funciones de activacion](../assets/activaciones.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hay muchas funciones de activacion.\n",
    "* Por definicion, las funciones de activacion:\n",
    "    * Son no lineales. Aplicaciones repetidas de $wx+b$ sin una funcion de activacion resultan en una polinomial. La no linealidad permite a la red aproximar funciones mas complejas.\n",
    "    * Son diferenciables, para poder calcular las gradientes a traves de ellas. Discontinuidades de punto como en `Hatdtanh` o `ReLU` son validas.\n",
    "* Sin esto, las redes caen a ser polinomiales complicadas o dificiles de entrenar.\n",
    "* Adicionalmente, las funciones:\n",
    "    * Tienen al menos un rango sensible, donde cambios no triviales en el input resultan en cambio no trivial correspondiente en el output\n",
    "    * Tienen al menos un rango no sensible (o saturado), donde cambios al input resultan en poco o ningun cambio en el output.\n",
    "* Por utlimo, las fuciones de activacion tienen al menos una de estas:\n",
    "    * Un limite inferior que se aproxima (o se encuentra) mientras el input tiende a negativo infinito.\n",
    "    * Un limite superior similar pero inverso para positivo infinito.\n",
    "* Dado lo que sabemos de como funciona back-propagation\n",
    "    * Sabemos que los errores se van a propagar hacia atras a traves de la activacion de manera mas efectiva cuando los inputs se encuentran dentro del rango de respuesta.\n",
    "    * Por otro lado, los errores no van a afectar a las neuornas para cuales el _input_ esta saturado debido a que la gradiente estara cercana a cero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En conclusion\n",
    "\n",
    "* En una red hecha de unidades lineales + activaciones, cuando recibe diferentes _inputs_:\n",
    "    * diferentes unidades van a responder en diferentes rangos para los mismos inputs\n",
    "    * los errores asociados a esos inputs van a afectar a las neuronas operancio en el rango sensible, dejando a las otras unidades mas o menos igual en el proceso de aprendizaje. \n",
    "* Juntar muchas operaciones lineales + unidades de activacion en paralelo y apilandolas una sobre otra nos provee un objeto matematico capaz de aproximar funciones complicadas. \n",
    "* Diferentes combinaciones de unidades van a responder a inputs en diferentes rangos\n",
    "    * Esos parametros son relativamente faciles de optimizar a traves de SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje para Neural Networks\n",
    "\n",
    "* Una red entrenada exitosamente, a traves de los valores de sus _weights_ y _biases_, va a capturar la estructura inherente de la data.\n",
    "* Esta estrcutura se captura en la forma de representaciones numericas significativas que funcionan de forma correcta para data que no ha visto anteriormente.\n",
    "* Los NNs nos proveen la habilidad de aproximar fenomenos altamente no lineales sin tener que tener un modelo explicito.\n",
    "    * En vez empezamos con un model generico, no entrenado y lo especializamos a una tarea al proveerle:\n",
    "        * un set de inputs\n",
    "        * un set de outputs\n",
    "        * una _loss function_ desde la cual puede realizar el back-propagation\n",
    "    * Especializar un modelo generico a una tarea especifica, usando ejemplos es a lo que nos referimos por _aprendizaje_\n",
    "    * El modelo no se construyo con esa tarea especifica en mente. No codificamos en el modelo reglas que describen como funciona la tarea.\n",
    "    \n",
    "    \n",
    "* Para nuestro modelo del termometro asumimos que las temperaturas se median de forma lineal.\n",
    "    * En esa suposicion implicitamente codificamos reglas para nuestra tarea: especificamos la forma de nuestra funcion input/output.\n",
    "    * No hubiesemos podido aproximar nada mas que no fueran puntos al rededor de una linea.\n",
    "* Mientras la dimensionalidad de un problema crece y las relaciones entre inputs/outputs se complican, asumir la forma de la funcion probablemente no va a funcionar.\n",
    "* De cierta forma estamos renunciando la interpretabilidad por la posibilidad de solucionar problemas mas complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El modulo `nn` de PyTorch\n",
    "\n",
    "* Vamos a reemplazar nuestro modelo lineal por un NN.\n",
    "* PyTorch tiene un submodulo entero dedicado a NNs llamado `torch.nn`\n",
    "* Contiene los bloques necesarios para construir todo tipo de arquitecturas de NNs.\n",
    "* Esos bloques se llaman _modules_ en PyTorch (en otros frameworks se llaman _layers_)\n",
    "* Un modulo de PyTorch es una clase de Python que deriva de la clase base `nn.Module`.\n",
    "    * Un modulo puede tener una o mas instancias de `Parameter` como atributos\n",
    "    * Estos son tensores cuyos valores son optimizados durante el proceso de entrenamiento ($w$ y $b$)\n",
    "    * Un modulo tambien puede tener uno o mas submodulos (subclases de `nn.Module` como atributos) y va a poder llevar un registro de sus `Parameter`s de igual forma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dibujo graficas computacionales separadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_t_un' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9371c2404f52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlinear_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlinear_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_t_un\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'val_t_un' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "linear_model = nn.Linear(1, 1)\n",
    "linear_model(val_t_un)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las subclases de `nn.Module` tienen un metodo `call` definido. Esto permite crear una instancia de `nn.Linear` y llamarla como si fuera una funcion.\n",
    "\n",
    "Llamar una instancia de `nn.Module` con un conjunto de argumetnos termina llamando un metodo llamado `forward` con esos mismos argumentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacion de `Module.call`\n",
    "\n",
    "(simplificado para claridad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-9-8b8724e9a76a>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-8b8724e9a76a>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    return result\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def __call__(self, *input, **kwargs):\n",
    "    for hook in self._forward_pre_hooks.values():\n",
    "        hook(self, input)\n",
    "        \n",
    "    result = self.forward(*input, **kwargs)\n",
    "    \n",
    "    for hook in self._forward_hooks.values():\n",
    "        hook_result = hook(self, input, result)\n",
    "        # ...\n",
    "        \n",
    "    for hook in self._backward_hooks.values():\n",
    "        # ...\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De regreso al modelo lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8318],\n",
       "        [4.7422]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "linear_model = nn.Linear(1, 1)\n",
    "linear_model(val_t_un)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Linear` acepta tres argumentos:\n",
    "* el numero de input features: size del input = 1\n",
    "* numero de output features: size del outpu = 1\n",
    "* si incluye un bias o no (por default es `True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(1)\n",
    "linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nuestro modelo toma un input y produce un output\n",
    "* `nn.Module` y sus subclases estan diseniados para hacer eso sobre multiples muestras al mismo tiempo\n",
    "* Para acomodar multiples muestras los modulos esperan que la dimension 0 del input sea el numero de muestras en un _batch_\n",
    "* Cualquier module en `nn` esta hecho para producir outputs para un _batch_ de multiples inputs al mismo tiempo.\n",
    "* B x Nin\n",
    "    * B es el tamanio del _batch_\n",
    "    * Nin el numero de input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(10, 1)\n",
    "linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para un dataset de imagenes:\n",
    "* BxCxHxW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0] # Temperatura en grados celsios\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4] # Unidades desconocidas\n",
    "t_c = torch.tensor(t_c).unsqueeze(1) # Agregamos una dimension para tener B x N_inputs\n",
    "t_u = torch.tensor(t_u).unsqueeze(1) # Agregamos una dimension para tener B x N_inputs\n",
    "\n",
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-86994c90cb88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mparams_old\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlearning_rate_old\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0moptimizer_old\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "params_old = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate_old = 1e-1\n",
    "optimizer_old = optim.Adam([params], lr=learning_rate)\n",
    "\n",
    "\n",
    "linear_model = nn.Linear(1, 1)\n",
    "optimizer = optim.SGD(\n",
    "    linear_model.parameters(), # reemplazamos [params] con este metodo \n",
    "    lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x0000026FF9552840>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.7044]], requires_grad=True), Parameter containing:\n",
       " tensor([-0.1855], requires_grad=True)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(linear_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, n_epochs, optimizer, loss_fn, train_x, val_x, train_y, val_y):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_x) # ya no tenemos que pasar los params\n",
    "        train_loss = loss_fn(train_t_p, train_y)\n",
    "        \n",
    "        with torch.no_grad(): # todos los args requires_grad=False\n",
    "            val_t_p = model(val_x)\n",
    "            val_loss = loss_fn(val_t_p, val_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch == 1 or epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss}, Validation loss {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 181.68565368652344, Validation loss 69.4239501953125\n",
      "Epoch 1000, Training loss 3.5213000774383545, Validation loss 7.0065484046936035\n",
      "Epoch 2000, Training loss 2.748034715652466, Validation loss 4.985044956207275\n",
      "Epoch 3000, Training loss 2.7254738807678223, Validation loss 4.728755950927734\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[5.3265]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-16.6941], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(1, 1)\n",
    "optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=3000,\n",
    "    optimizer=optimizer,\n",
    "    model=linear_model,\n",
    "    loss_fn=nn.MSELoss(), # Ya no estamos usando nuestra loss function hecha a mano\n",
    "    train_x = train_t_un,\n",
    "    val_x = val_t_un,\n",
    "    train_y = train_t_c,\n",
    "    val_y = val_t_c)\n",
    "\n",
    "print()\n",
    "print(linear_model.weight)\n",
    "print(linear_model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalmente un Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ultimo paso: reemplazar nuestro modelo lineal\n",
    "* No va a ser mejor\n",
    "* Lo unico que vamos a cambiar va a ser el modelo\n",
    "* Un simple NN:\n",
    "    * Una capa lineal\n",
    "    * Activacion\n",
    "    * \"hidden layers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=13, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=13, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model = nn.Sequential(\n",
    "                nn.Linear(1, 13), # El 13 es arbitrario\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(13, 1) # Este 13 debe hacer match con el primero\n",
    "            )\n",
    "\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El resultado final es un modelo que toma los inputs esperados por el primer modulo (_layer_)\n",
    "* Pasa los outputs intermedios al resto de los modulos\n",
    "* Produce un output retornado por el ultimo modulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([13, 1]), torch.Size([13]), torch.Size([1, 13]), torch.Size([1])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param.size() for param in seq_model.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Estos son los parametros que el optimizador va a recibir\n",
    "* Al llamar `backward()` todos los parametros se van a llenar con su `grad`\n",
    "* El optimizador va a actualizar el valor de `grad` durante `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([13, 1])\n",
      "0.bias torch.Size([13])\n",
      "2.weight torch.Size([1, 13])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in seq_model.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=13, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=13, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "named_seq_model = nn.Sequential(OrderedDict([\n",
    "        ('hidden_linear', nn.Linear(1, 8)),\n",
    "        ('hidden_activation', nn.Tanh()),\n",
    "        ('output_linear', nn.Linear(8, 1))\n",
    "]))\n",
    "\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_linear.weight torch.Size([8, 1])\n",
      "hidden_linear.bias torch.Size([8])\n",
      "output_linear.weight torch.Size([1, 8])\n",
      "output_linear.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in named_seq_model.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.1299], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_seq_model.output_linear.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Util para inspeccionar parametros o sus gradientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 208.18772888183594, Validation loss 83.814697265625\n",
      "Epoch 1000, Training loss 3.8366997241973877, Validation loss 4.554821491241455\n",
      "Epoch 2000, Training loss 2.791919708251953, Validation loss 1.495053768157959\n",
      "Epoch 3000, Training loss 1.7354196310043335, Validation loss 2.7989323139190674\n",
      "Epoch 4000, Training loss 1.6313438415527344, Validation loss 2.9179413318634033\n",
      "Epoch 5000, Training loss 1.5320508480072021, Validation loss 3.2044100761413574\n",
      "output tensor([[ 2.1565],\n",
      "        [15.9460]], grad_fn=<AddmmBackward>)\n",
      "answer tensor([[ 0.5000],\n",
      "        [13.0000]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'hidden_linear'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-02a1e5a48953>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_t_un\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'answer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_t_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hidden'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_linear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\ai1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    537\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 539\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'hidden_linear'"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=5000,\n",
    "    optimizer=optimizer,\n",
    "    model=seq_model,\n",
    "    loss_fn=nn.MSELoss(), # Ya no estamos usando nuestra loss function hecha a mano\n",
    "    train_x = train_t_un,\n",
    "    val_x = val_t_un,\n",
    "    train_y = train_t_c,\n",
    "    val_y = val_t_c)\n",
    "\n",
    "print('output', seq_model(val_t_un))\n",
    "print('answer', val_t_c)\n",
    "print('hidden', seq_model.hidden_linear.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien podemos evaluar el modelo en toda la data y ver que tan diferente es de una linea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 3840x2880 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "t_range = torch.arange(20., 90.).unsqueeze(1)\n",
    "\n",
    "fig = plt.figure(dpi=600)\n",
    "plt.xlabel(\"Fahrenheit\")\n",
    "plt.ylabel(\"Celsius\")\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')\n",
    "plt.plot(t_range.numpy(), seq_model(0.1 * t_range).detach().numpy(), 'c-')\n",
    "plt.plot(t_u.numpy(), seq_model(0.1 * t_u).detach().numpy(), 'kx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclassing nn.Module\n",
    "\n",
    "* sublcassing `nn.Module` nos da mucha mas flexibilidad.\n",
    "* La interface especifica que como minimo debemos definir un metodo `forward` para la subclase\n",
    "    * `forward` toma el input al model y regresa el output\n",
    "* Si usamos las operaciones de `torch`, `autograd` se encarga de hacer el `backward` pass de forma automatica\n",
    "\n",
    "* Normalmente vamos a definir los submodulos que usamos en el metodo `forward` en el constructor\n",
    "    * Esto permite que sean llamados en `forward` y que puedan mantener sus parametros a durante la existencia de nuestro modulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SubclassModel(\n",
       "  (hidden_linear): Linear(in_features=1, out_features=13, bias=True)\n",
       "  (hidden_activation): Tanh()\n",
       "  (output_linear): Linear(in_features=13, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SubclassModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_linear = nn.Linear(1, 13)\n",
    "        self.hidden_activation = nn.Tanh()\n",
    "        self.output_linear = nn.Linear(13, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, input):\n",
    "        hidden_t = self.hidden_linear(input)\n",
    "        activated_t = self.hidden_activation(hidden_t)\n",
    "        #activated_t = self.hidden_activation(hidden_t) if random.random() > 0.5 else hidden_t\n",
    "        output_t = self.output_linear(activated_t)\n",
    "\n",
    "        return output_t\n",
    "\n",
    "    \n",
    "subclass_model = SubclassModel()\n",
    "subclass_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nos permite manipular los outputs de forma directa  y transformarlo en un tensor BxN\n",
    "* Dejamos la dimension de batch como -1 ya que no sabemos cuantos inputs van a venir por batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Asignar una instancia de `nn.Module` a un atributo en un `nn.Module` registra el modulo como un submodulo.\n",
    "* Permite a `Net` acceso a los `parameters` de sus submodulos sin necesidad de hacerlo manualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, [13, 13, 13, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in subclass_model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lo que paso**\n",
    "\n",
    "* `parameters()` investiga todos los submodulos asignados como atributos del constructor y llama `parameters` de forma recursiva.\n",
    "* Al accesar su atributo `grad`, el cual va a ser llenado por el `autograd`, el optimizador va a saber como cambiar los parametros para minimizar el _loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq\n",
      "0.weight              torch.Size([13, 1]) 13\n",
      "0.bias                torch.Size([13])    13\n",
      "2.weight              torch.Size([1, 13]) 13\n",
      "2.bias                torch.Size([1])     1\n",
      "\n",
      "named_seq\n",
      "hidden_linear.weight  torch.Size([8, 1])  8\n",
      "hidden_linear.bias    torch.Size([8])     8\n",
      "output_linear.weight  torch.Size([1, 8])  8\n",
      "output_linear.bias    torch.Size([1])     1\n",
      "\n",
      "subclass\n",
      "hidden_linear.weight  torch.Size([13, 1]) 13\n",
      "hidden_linear.bias    torch.Size([13])    13\n",
      "output_linear.weight  torch.Size([1, 13]) 13\n",
      "output_linear.bias    torch.Size([1])     1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for type_str, model in [('seq', seq_model), ('named_seq', named_seq_model), ('subclass', subclass_model)]:\n",
    "    print(type_str)\n",
    "    for name_str, param in model.named_parameters():\n",
    "        print(\"{:21} {:19} {}\".format(name_str, str(param.shape), param.numel()))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SubclassFunctionalModel(\n",
       "  (hidden_linear): Linear(in_features=1, out_features=14, bias=True)\n",
       "  (output_linear): Linear(in_features=14, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SubclassFunctionalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_linear = nn.Linear(1, 14)\n",
    "        self.output_linear = nn.Linear(14, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, input):\n",
    "        hidden_t = self.hidden_linear(input)\n",
    "        activated_t = torch.tanh(hidden_t)\n",
    "        output_t = self.output_linear(activated_t)\n",
    "\n",
    "        return output_t\n",
    "\n",
    "\n",
    "func_model = SubclassFunctionalModel()\n",
    "func_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Experimenten con el numero de neuronas en el modelo al igual que el learning rate.\n",
    "    * Que cambios resultan en un output mas lineal del modelo?\n",
    "    * Pueden hacer que el modelo haga un overfit obvio de la data?\n",
    "    \n",
    "* Cargen la [data de vinos blancos](https://archive.ics.uci.edu/ml/datasets/wine+quality) y creen un modelo con el numero apropiado de inputs\n",
    "    * Cuanto tarda en entrenar comparado al dataset que hemos estado usando?\n",
    "    * Pueden explicar que factores contribuyen a los tiempos de entrenamiento?\n",
    "    * Pueden hacer que el _loss_ disminuya?\n",
    "    * Intenten graficar la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 236.01341247558594, Validation loss 100.09624481201172\n",
      "Epoch 1000, Training loss 209.72579956054688, Validation loss 84.43618774414062\n",
      "Epoch 2000, Training loss 191.0543975830078, Validation loss 73.62203216552734\n",
      "Epoch 3000, Training loss 175.98362731933594, Validation loss 65.41678619384766\n",
      "Epoch 4000, Training loss 161.8820037841797, Validation loss 58.13094711303711\n",
      "Epoch 5000, Training loss 149.08676147460938, Validation loss 51.984920501708984\n"
     ]
    }
   ],
   "source": [
    "seq_model = nn.Sequential(\n",
    "                nn.Linear(1, 3), # El 13 es arbitrario\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(3, 1) # Este 13 debe hacer match con el primero\n",
    "            )\n",
    "\n",
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-5)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=5000,\n",
    "    optimizer=optimizer,\n",
    "    model=seq_model,\n",
    "    loss_fn=nn.MSELoss(), # Ya no estamos usando nuestra loss function hecha a mano\n",
    "    train_x = train_t_un,\n",
    "    val_x = val_t_un,\n",
    "    train_y = train_t_c,\n",
    "    val_y = val_t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 191.3069610595703, Validation loss 73.87039184570312\n",
      "Epoch 1000, Training loss 70.83070373535156, Validation loss 34.170345306396484\n",
      "Epoch 2000, Training loss 40.78306198120117, Validation loss 18.920591354370117\n",
      "Epoch 3000, Training loss 25.78692054748535, Validation loss 11.215924263000488\n",
      "Epoch 4000, Training loss 17.530498504638672, Validation loss 6.7094855308532715\n",
      "Epoch 5000, Training loss 13.156546592712402, Validation loss 4.894273281097412\n"
     ]
    }
   ],
   "source": [
    "seq_model = nn.Sequential(\n",
    "                nn.Linear(1, 7), # El 13 es arbitrario\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(7, 1) # Este 13 debe hacer match con el primero\n",
    "            )\n",
    "\n",
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-4)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=5000,\n",
    "    optimizer=optimizer,\n",
    "    model=seq_model,\n",
    "    loss_fn=nn.MSELoss(), # Ya no estamos usando nuestra loss function hecha a mano\n",
    "    train_x = train_t_un,\n",
    "    val_x = val_t_un,\n",
    "    train_y = train_t_c,\n",
    "    val_y = val_t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"winequality-white.csv\", sep=\";\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fixed acidity</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022697</td>\n",
       "      <td>0.289181</td>\n",
       "      <td>0.089021</td>\n",
       "      <td>0.023086</td>\n",
       "      <td>-0.049396</td>\n",
       "      <td>0.091070</td>\n",
       "      <td>0.265331</td>\n",
       "      <td>-0.425858</td>\n",
       "      <td>-0.017143</td>\n",
       "      <td>-0.120881</td>\n",
       "      <td>-0.113663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatile acidity</th>\n",
       "      <td>-0.022697</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.149472</td>\n",
       "      <td>0.064286</td>\n",
       "      <td>0.070512</td>\n",
       "      <td>-0.097012</td>\n",
       "      <td>0.089261</td>\n",
       "      <td>0.027114</td>\n",
       "      <td>-0.031915</td>\n",
       "      <td>-0.035728</td>\n",
       "      <td>0.067718</td>\n",
       "      <td>-0.194723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citric acid</th>\n",
       "      <td>0.289181</td>\n",
       "      <td>-0.149472</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.094212</td>\n",
       "      <td>0.114364</td>\n",
       "      <td>0.094077</td>\n",
       "      <td>0.121131</td>\n",
       "      <td>0.149503</td>\n",
       "      <td>-0.163748</td>\n",
       "      <td>0.062331</td>\n",
       "      <td>-0.075729</td>\n",
       "      <td>-0.009209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>residual sugar</th>\n",
       "      <td>0.089021</td>\n",
       "      <td>0.064286</td>\n",
       "      <td>0.094212</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.088685</td>\n",
       "      <td>0.299098</td>\n",
       "      <td>0.401439</td>\n",
       "      <td>0.838966</td>\n",
       "      <td>-0.194133</td>\n",
       "      <td>-0.026664</td>\n",
       "      <td>-0.450631</td>\n",
       "      <td>-0.097577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chlorides</th>\n",
       "      <td>0.023086</td>\n",
       "      <td>0.070512</td>\n",
       "      <td>0.114364</td>\n",
       "      <td>0.088685</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.101392</td>\n",
       "      <td>0.198910</td>\n",
       "      <td>0.257211</td>\n",
       "      <td>-0.090439</td>\n",
       "      <td>0.016763</td>\n",
       "      <td>-0.360189</td>\n",
       "      <td>-0.209934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <td>-0.049396</td>\n",
       "      <td>-0.097012</td>\n",
       "      <td>0.094077</td>\n",
       "      <td>0.299098</td>\n",
       "      <td>0.101392</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615501</td>\n",
       "      <td>0.294210</td>\n",
       "      <td>-0.000618</td>\n",
       "      <td>0.059217</td>\n",
       "      <td>-0.250104</td>\n",
       "      <td>0.008158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <td>0.091070</td>\n",
       "      <td>0.089261</td>\n",
       "      <td>0.121131</td>\n",
       "      <td>0.401439</td>\n",
       "      <td>0.198910</td>\n",
       "      <td>0.615501</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.529881</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.134562</td>\n",
       "      <td>-0.448892</td>\n",
       "      <td>-0.174737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>density</th>\n",
       "      <td>0.265331</td>\n",
       "      <td>0.027114</td>\n",
       "      <td>0.149503</td>\n",
       "      <td>0.838966</td>\n",
       "      <td>0.257211</td>\n",
       "      <td>0.294210</td>\n",
       "      <td>0.529881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.093591</td>\n",
       "      <td>0.074493</td>\n",
       "      <td>-0.780138</td>\n",
       "      <td>-0.307123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pH</th>\n",
       "      <td>-0.425858</td>\n",
       "      <td>-0.031915</td>\n",
       "      <td>-0.163748</td>\n",
       "      <td>-0.194133</td>\n",
       "      <td>-0.090439</td>\n",
       "      <td>-0.000618</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>-0.093591</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.155951</td>\n",
       "      <td>0.121432</td>\n",
       "      <td>0.099427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sulphates</th>\n",
       "      <td>-0.017143</td>\n",
       "      <td>-0.035728</td>\n",
       "      <td>0.062331</td>\n",
       "      <td>-0.026664</td>\n",
       "      <td>0.016763</td>\n",
       "      <td>0.059217</td>\n",
       "      <td>0.134562</td>\n",
       "      <td>0.074493</td>\n",
       "      <td>0.155951</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.017433</td>\n",
       "      <td>0.053678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alcohol</th>\n",
       "      <td>-0.120881</td>\n",
       "      <td>0.067718</td>\n",
       "      <td>-0.075729</td>\n",
       "      <td>-0.450631</td>\n",
       "      <td>-0.360189</td>\n",
       "      <td>-0.250104</td>\n",
       "      <td>-0.448892</td>\n",
       "      <td>-0.780138</td>\n",
       "      <td>0.121432</td>\n",
       "      <td>-0.017433</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.435575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>-0.113663</td>\n",
       "      <td>-0.194723</td>\n",
       "      <td>-0.009209</td>\n",
       "      <td>-0.097577</td>\n",
       "      <td>-0.209934</td>\n",
       "      <td>0.008158</td>\n",
       "      <td>-0.174737</td>\n",
       "      <td>-0.307123</td>\n",
       "      <td>0.099427</td>\n",
       "      <td>0.053678</td>\n",
       "      <td>0.435575</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      fixed acidity  volatile acidity  citric acid  \\\n",
       "fixed acidity              1.000000         -0.022697     0.289181   \n",
       "volatile acidity          -0.022697          1.000000    -0.149472   \n",
       "citric acid                0.289181         -0.149472     1.000000   \n",
       "residual sugar             0.089021          0.064286     0.094212   \n",
       "chlorides                  0.023086          0.070512     0.114364   \n",
       "free sulfur dioxide       -0.049396         -0.097012     0.094077   \n",
       "total sulfur dioxide       0.091070          0.089261     0.121131   \n",
       "density                    0.265331          0.027114     0.149503   \n",
       "pH                        -0.425858         -0.031915    -0.163748   \n",
       "sulphates                 -0.017143         -0.035728     0.062331   \n",
       "alcohol                   -0.120881          0.067718    -0.075729   \n",
       "quality                   -0.113663         -0.194723    -0.009209   \n",
       "\n",
       "                      residual sugar  chlorides  free sulfur dioxide  \\\n",
       "fixed acidity               0.089021   0.023086            -0.049396   \n",
       "volatile acidity            0.064286   0.070512            -0.097012   \n",
       "citric acid                 0.094212   0.114364             0.094077   \n",
       "residual sugar              1.000000   0.088685             0.299098   \n",
       "chlorides                   0.088685   1.000000             0.101392   \n",
       "free sulfur dioxide         0.299098   0.101392             1.000000   \n",
       "total sulfur dioxide        0.401439   0.198910             0.615501   \n",
       "density                     0.838966   0.257211             0.294210   \n",
       "pH                         -0.194133  -0.090439            -0.000618   \n",
       "sulphates                  -0.026664   0.016763             0.059217   \n",
       "alcohol                    -0.450631  -0.360189            -0.250104   \n",
       "quality                    -0.097577  -0.209934             0.008158   \n",
       "\n",
       "                      total sulfur dioxide   density        pH  sulphates  \\\n",
       "fixed acidity                     0.091070  0.265331 -0.425858  -0.017143   \n",
       "volatile acidity                  0.089261  0.027114 -0.031915  -0.035728   \n",
       "citric acid                       0.121131  0.149503 -0.163748   0.062331   \n",
       "residual sugar                    0.401439  0.838966 -0.194133  -0.026664   \n",
       "chlorides                         0.198910  0.257211 -0.090439   0.016763   \n",
       "free sulfur dioxide               0.615501  0.294210 -0.000618   0.059217   \n",
       "total sulfur dioxide              1.000000  0.529881  0.002321   0.134562   \n",
       "density                           0.529881  1.000000 -0.093591   0.074493   \n",
       "pH                                0.002321 -0.093591  1.000000   0.155951   \n",
       "sulphates                         0.134562  0.074493  0.155951   1.000000   \n",
       "alcohol                          -0.448892 -0.780138  0.121432  -0.017433   \n",
       "quality                          -0.174737 -0.307123  0.099427   0.053678   \n",
       "\n",
       "                       alcohol   quality  \n",
       "fixed acidity        -0.120881 -0.113663  \n",
       "volatile acidity      0.067718 -0.194723  \n",
       "citric acid          -0.075729 -0.009209  \n",
       "residual sugar       -0.450631 -0.097577  \n",
       "chlorides            -0.360189 -0.209934  \n",
       "free sulfur dioxide  -0.250104  0.008158  \n",
       "total sulfur dioxide -0.448892 -0.174737  \n",
       "density              -0.780138 -0.307123  \n",
       "pH                    0.121432  0.099427  \n",
       "sulphates            -0.017433  0.053678  \n",
       "alcohol               1.000000  0.435575  \n",
       "quality               0.435575  1.000000  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = df.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['residual sugar']\n",
    "y = df['density']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train = torch.tensor(X_train).unsqueeze(1)\n",
    "X_test = torch.tensor(X_test).unsqueeze(1)\n",
    "y_train = torch.tensor(y_train).unsqueeze(1)\n",
    "y_test = torch.tensor(y_test).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 1.2188410758972168, Validation loss 1.2255860567092896\n",
      "Epoch 1000, Training loss 0.1352592408657074, Validation loss 0.13732923567295074\n",
      "Epoch 2000, Training loss 0.04941604658961296, Validation loss 0.049583472311496735\n",
      "Epoch 3000, Training loss 0.03478197753429413, Validation loss 0.03463069722056389\n",
      "Epoch 4000, Training loss 0.027955850586295128, Validation loss 0.027814671397209167\n",
      "Epoch 5000, Training loss 0.02335743047297001, Validation loss 0.023266686126589775\n"
     ]
    }
   ],
   "source": [
    "seq_model = nn.Sequential(\n",
    "                nn.Linear(1, 7), \n",
    "                nn.Tanh(),\n",
    "                nn.Linear(7, 1) \n",
    "            )\n",
    "\n",
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-4)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=5000,\n",
    "    optimizer=optimizer,\n",
    "    model=seq_model,\n",
    "    loss_fn=nn.MSELoss(), \n",
    "    train_x = X_train.float(),\n",
    "    val_x = X_test.float(),\n",
    "    train_y = y_train.float(),\n",
    "    val_y = y_test.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables utilizadas fueron \"residual sugar\" para poder predecir la densidad de los vinos. Con diferencia a la data que estabamos utilizando el loss disminuye bastante debido a la alta relacion que tienen las variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
