{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios\n",
    "\n",
    "## Nathalia Morales\n",
    "\n",
    "1. Experimenten con el numero de neuronas en el modelo al igual que el learning rate.\n",
    "    \n",
    "    a. Que cambios resultan en un output mas lineal del modelo?\n",
    "    \n",
    "    b. Pueden hacer que el modelo haga un overfit obvio de la data?\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "2. Cargen la [data de vinos blancos](https://archive.ics.uci.edu/ml/datasets/wine+quality) y creen un modelo con el numero apropiado de inputs\n",
    "    \n",
    "    a. Cuanto tarda en entrenar comparado al dataset que hemos estado usando?\n",
    "    \n",
    "    b. Pueden explicar que factores contribuyen a los tiempos de entrenamiento?\n",
    "    \n",
    "    c. Pueden hacer que el _loss_ disminuya?\n",
    "    \n",
    "    d. Intenten graficar la data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREGUNTA 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0] # Temperatura en grados celsios\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4] # Unidades desconocidas\n",
    "t_c = torch.tensor(t_c).unsqueeze(1) # Agregamos una dimension para tener B x N_inputs\n",
    "t_u = torch.tensor(t_u).unsqueeze(1) # Agregamos una dimension para tener B x N_inputs\n",
    "\n",
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, n_epochs, optimizer, loss_fn, train_x, val_x, train_y, val_y):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_x)\n",
    "        train_loss = loss_fn(train_t_p, train_y)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_t_p = model(val_x)\n",
    "            val_loss = loss_fn(val_t_p, val_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch == 1 or epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss}, Validation loss {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubclassModel(nn.Module):\n",
    "    def __init__(self, num):\n",
    "        super().__init__()\n",
    "        self.hidden_linear = nn.Linear(1, num)\n",
    "        self.hidden_activation = nn.Tanh()\n",
    "        self.output_linear = nn.Linear(num, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden_t = self.hidden_linear(input)\n",
    "        activated_t = self.hidden_activation(hidden_t)\n",
    "        output_t = self.output_linear(activated_t)\n",
    "\n",
    "        return output_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 207.57418823242188, Validation loss 56.70370864868164\n",
      "Epoch 1000, Training loss 4.345487594604492, Validation loss 3.6638436317443848\n",
      "Epoch 2000, Training loss 4.191321849822998, Validation loss 8.117053031921387\n",
      "Epoch 3000, Training loss 2.768686532974243, Validation loss 5.3557562828063965\n",
      "Epoch 4000, Training loss 2.3527543544769287, Validation loss 4.536910533905029\n",
      "Epoch 5000, Training loss 2.0870046615600586, Validation loss 3.90848970413208\n",
      "output tensor([[ 1.9058],\n",
      "        [12.0217]], grad_fn=<AddmmBackward>)\n",
      "answer tensor([[ 0.5000],\n",
      "        [11.0000]])\n",
      "hidden tensor([[  0.8088],\n",
      "        [ 12.7127],\n",
      "        [  0.0811],\n",
      "        [-11.4233],\n",
      "        [ -0.1161],\n",
      "        [-12.2943],\n",
      "        [ -0.0471],\n",
      "        [-13.5199],\n",
      "        [ -8.8895],\n",
      "        [  8.7191],\n",
      "        [ -2.4318],\n",
      "        [  0.1213],\n",
      "        [-11.8224]])\n"
     ]
    }
   ],
   "source": [
    "subclass_model = SubclassModel(13)\n",
    "\n",
    "optimizer = optim.SGD(subclass_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=5000,\n",
    "    optimizer=optimizer,\n",
    "    model=subclass_model,\n",
    "    loss_fn=nn.MSELoss(), # Ya no estamos usando nuestra loss function hecha a mano\n",
    "    train_x = train_t_un,\n",
    "    val_x = val_t_un,\n",
    "    train_y = train_t_c,\n",
    "    val_y = val_t_c)\n",
    "\n",
    "print('output', subclass_model(val_t_un))\n",
    "print('answer', val_t_c)\n",
    "print('hidden', subclass_model.hidden_linear.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 199.02017211914062, Validation loss 52.30779266357422\n",
      "Epoch 1000, Training loss 7.619917392730713, Validation loss 3.8969128131866455\n",
      "Epoch 2000, Training loss 6.849825382232666, Validation loss 11.925092697143555\n",
      "Epoch 3000, Training loss 3.305816411972046, Validation loss 6.030238628387451\n",
      "Epoch 4000, Training loss 2.22808575630188, Validation loss 4.1533331871032715\n",
      "Epoch 5000, Training loss 1.9908980131149292, Validation loss 4.028945446014404\n",
      "output tensor([[ 2.1724],\n",
      "        [11.8699]], grad_fn=<AddmmBackward>)\n",
      "answer tensor([[ 0.5000],\n",
      "        [11.0000]])\n",
      "hidden tensor([[ -0.3298],\n",
      "        [ 19.4053],\n",
      "        [ 14.1897],\n",
      "        [ -0.5535],\n",
      "        [-16.9487]])\n"
     ]
    }
   ],
   "source": [
    "subclass_model = SubclassModel(5)\n",
    "\n",
    "optimizer = optim.SGD(subclass_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=5000,\n",
    "    optimizer=optimizer,\n",
    "    model=subclass_model,\n",
    "    loss_fn=nn.MSELoss(), # Ya no estamos usando nuestra loss function hecha a mano\n",
    "    train_x = train_t_un,\n",
    "    val_x = val_t_un,\n",
    "    train_y = train_t_c,\n",
    "    val_y = val_t_c)\n",
    "\n",
    "print('output', subclass_model(val_t_un))\n",
    "print('answer', val_t_c)\n",
    "print('hidden', subclass_model.hidden_linear.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25 layers\n",
    "* Tiende a overfit (1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 234.3259735107422, Validation loss 70.40892028808594\n",
      "Epoch 1000, Training loss 3.1621735095977783, Validation loss 3.604989767074585\n",
      "Epoch 2000, Training loss 3.053976535797119, Validation loss 0.8564319014549255\n",
      "Epoch 3000, Training loss 2.3069705963134766, Validation loss 1.2977840900421143\n",
      "Epoch 4000, Training loss 2.063998222351074, Validation loss 1.5867054462432861\n",
      "Epoch 5000, Training loss 1.9696531295776367, Validation loss 1.7700603008270264\n",
      "output tensor([[ 2.2175],\n",
      "        [12.9766]], grad_fn=<AddmmBackward>)\n",
      "answer tensor([[ 0.5000],\n",
      "        [11.0000]])\n",
      "hidden tensor([[ 2.9764e-03],\n",
      "        [ 5.0816e-01],\n",
      "        [ 2.1858e-02],\n",
      "        [ 8.3383e-01],\n",
      "        [ 5.9357e+00],\n",
      "        [-7.8046e+00],\n",
      "        [-4.4292e-01],\n",
      "        [-8.1456e+00],\n",
      "        [ 2.3645e-03],\n",
      "        [ 8.6857e-04],\n",
      "        [-6.7006e+00],\n",
      "        [ 1.9460e-02],\n",
      "        [-3.2586e-01],\n",
      "        [ 4.4285e+00],\n",
      "        [ 8.4881e+00],\n",
      "        [-1.3229e+00],\n",
      "        [-7.1200e-03],\n",
      "        [ 8.1314e-02],\n",
      "        [-6.6886e-01],\n",
      "        [-2.6814e-01],\n",
      "        [-3.1269e+00],\n",
      "        [-4.7357e-01],\n",
      "        [-7.7919e+00],\n",
      "        [-5.4498e+00],\n",
      "        [-7.5483e+00]])\n"
     ]
    }
   ],
   "source": [
    "subclass_model = SubclassModel(25)\n",
    "\n",
    "optimizer = optim.SGD(subclass_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=5000,\n",
    "    optimizer=optimizer,\n",
    "    model=subclass_model,\n",
    "    loss_fn=nn.MSELoss(), # Ya no estamos usando nuestra loss function hecha a mano\n",
    "    train_x = train_t_un,\n",
    "    val_x = val_t_un,\n",
    "    train_y = train_t_c,\n",
    "    val_y = val_t_c)\n",
    "\n",
    "print('output', subclass_model(val_t_un))\n",
    "print('answer', val_t_c)\n",
    "print('hidden', subclass_model.hidden_linear.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mientras mas incrementamos el numero de layers menor se convierte el error pero tambien tiende a un overfit de la data. Para poder tener un modelo \"mas linear\" tendriamos que inclinarnos a encontrar un numero de layers que adecue para no hace uner o over-fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREGUNTA 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargando la data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Cuanto tarda en entrenar comparado al dataset que hemos estado usando?\n",
    "    * No tarda mucho, dado que se carga desde numpy.\n",
    "    \n",
    "b. Pueden explicar que factores contribuyen a los tiempos de entrenamiento?\n",
    "    * El tamano de la data. El tipo de estructura de data.\n",
    "    \n",
    "c. Pueden hacer que el loss disminuya?\n",
    "    * \n",
    "    \n",
    "d. Intenten graficar la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.000e+00 2.700e-01 3.600e-01 2.070e+01 4.500e-02 4.500e+01 1.700e+02\n",
      "  1.001e+00 3.000e+00 4.500e-01 8.800e+00 6.000e+00]\n",
      " [6.300e+00 3.000e-01 3.400e-01 1.600e+00 4.900e-02 1.400e+01 1.320e+02\n",
      "  9.940e-01 3.300e+00 4.900e-01 9.500e+00 6.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.loadtxt('../data/tabular-wine/winequality-white.csv', delimiter=\";\", skiprows=1).astype(float)\n",
    "#print(type(data))\n",
    "print (data[0:2,:]) # first 2 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "x = data[:,0:11]\n",
    "y = data[:,-1]\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "x = x.float()\n",
    "y = y.float()\n",
    "\n",
    "x = torch.tensor(x).unsqueeze(1) \n",
    "y = torch.tensor(y).unsqueeze(1)\n",
    "\n",
    "n_samples = data.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "train_x = x[:n_val]\n",
    "train_y = y[:n_val]\n",
    "\n",
    "val_x = x[n_val:]\n",
    "val_y = y[n_val:]\n",
    "\n",
    "CrossEntropyLoss = torch.nn.CrossEntropyLoss()\n",
    "#train_t_un = 0.1 * train_t_u\n",
    "#val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, n_epochs, optimizer, loss_fn, train_x, val_x, train_y, val_y):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_x)\n",
    "        train_loss = loss_fn(train_t_p, train_y)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_t_p = model(val_x)\n",
    "            val_loss = loss_fn(val_t_p, val_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch == 1 or epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss}, Validation loss {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubclassModel(nn.Module):\n",
    "    def __init__(self, num):\n",
    "        super().__init__()\n",
    "        self.hidden_linear = nn.Linear(1, num)\n",
    "        self.hidden_activation = nn.Tanh()\n",
    "        self.output_linear = nn.Linear(num, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden_t = self.hidden_linear(input)\n",
    "        activated_t = self.hidden_activation(hidden_t)\n",
    "        output_t = self.output_linear(activated_t)\n",
    "        torch.softmax(x,dim=1)\n",
    "\n",
    "        return output_t\n",
    "lf = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corriendo las funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con 25 Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 225.99929809570312, Validation loss 65.46632385253906\n",
      "Epoch 1000, Training loss 3.2555410861968994, Validation loss 3.627790689468384\n",
      "Epoch 2000, Training loss 3.222292423248291, Validation loss 0.7058439254760742\n",
      "Epoch 3000, Training loss 2.393249273300171, Validation loss 1.1475290060043335\n",
      "Epoch 4000, Training loss 2.1433265209198, Validation loss 1.405439853668213\n",
      "Epoch 5000, Training loss 2.0343775749206543, Validation loss 1.5787097215652466\n",
      "output tensor([[ 2.1963],\n",
      "        [13.0570]], grad_fn=<AddmmBackward>)\n",
      "answer tensor([[ 0.5000],\n",
      "        [11.0000]])\n",
      "hidden tensor([[ 7.1845e-02],\n",
      "        [ 7.1077e-01],\n",
      "        [ 8.3568e-01],\n",
      "        [ 3.0976e-03],\n",
      "        [-8.5702e-01],\n",
      "        [-9.4307e+00],\n",
      "        [ 4.3290e-03],\n",
      "        [ 7.9703e+00],\n",
      "        [-7.5243e+00],\n",
      "        [-2.9198e-01],\n",
      "        [ 2.2652e-01],\n",
      "        [ 8.3157e+00],\n",
      "        [ 8.0844e-01],\n",
      "        [ 7.6001e-03],\n",
      "        [ 5.7672e-01],\n",
      "        [-9.3999e+00],\n",
      "        [ 9.1233e-01],\n",
      "        [ 1.0513e-01],\n",
      "        [-6.6416e+00],\n",
      "        [-1.0493e+01],\n",
      "        [ 6.7510e+00],\n",
      "        [-2.6147e-02],\n",
      "        [ 1.0363e-01],\n",
      "        [-1.4673e-03],\n",
      "        [-1.0008e+01]])\n"
     ]
    }
   ],
   "source": [
    "subclass_model = SubclassModel(25)\n",
    "\n",
    "optimizer = optim.SGD(subclass_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=5000,\n",
    "    optimizer=optimizer,\n",
    "    model=subclass_model,\n",
    "    loss_fn=nn.MSELoss(), # Ya no estamos usando nuestra loss function hecha a mano\n",
    "    train_x = train_t_un,\n",
    "    val_x = val_t_un,\n",
    "    train_y = train_t_c,\n",
    "    val_y = val_t_c)\n",
    "\n",
    "print('output', subclass_model(val_t_un))\n",
    "print('answer', val_t_c)\n",
    "print('hidden', subclass_model.hidden_linear.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 214.64125061035156, Validation loss 59.96287536621094\n",
      "Epoch 1000, Training loss 4.819217205047607, Validation loss 3.426779270172119\n",
      "Epoch 2000, Training loss 5.057486534118652, Validation loss 9.091812133789062\n",
      "Epoch 3000, Training loss 3.3582987785339355, Validation loss 6.536432266235352\n",
      "Epoch 4000, Training loss 2.20505428314209, Validation loss 4.019712924957275\n",
      "Epoch 5000, Training loss 2.1684963703155518, Validation loss 4.423313140869141\n",
      "output tensor([[ 2.0653],\n",
      "        [11.8438]], grad_fn=<AddmmBackward>)\n",
      "answer tensor([[ 0.5000],\n",
      "        [11.0000]])\n",
      "hidden tensor([[  0.2115],\n",
      "        [ 17.9567],\n",
      "        [ -3.9392],\n",
      "        [  0.8184],\n",
      "        [ 16.3162],\n",
      "        [-17.2717],\n",
      "        [ -0.0281],\n",
      "        [-13.7747],\n",
      "        [ 17.8609],\n",
      "        [  0.0566]])\n"
     ]
    }
   ],
   "source": [
    "subclass_model = SubclassModel(10)\n",
    "\n",
    "optimizer = optim.SGD(subclass_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=5000,\n",
    "    optimizer=optimizer,\n",
    "    model=subclass_model,\n",
    "    loss_fn=nn.MSELoss(), # Ya no estamos usando nuestra loss function hecha a mano\n",
    "    train_x = train_t_un,\n",
    "    val_x = val_t_un,\n",
    "    train_y = train_t_c,\n",
    "    val_y = val_t_c)\n",
    "\n",
    "print('output', subclass_model(val_t_un))\n",
    "print('answer', val_t_c)\n",
    "print('hidden', subclass_model.hidden_linear.weight.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
