# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K6jwY1ZjFKYH7rj3zLNBFJNZDSFiE3kJ
"""

import numpy as np 
import pandas as pd

import torch
from torchtext import data

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

SEED = 1234

torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

TEXT = data.Field(tokenize = 'spacy') # se declara como se hara la toknizacion
LABEL = data.LabelField(dtype = torch.float) # se declara que tipo de dato es la variables que se desea analisar

from torchtext import datasets

train_data, test_data = datasets.IMDB.splits(TEXT, LABEL) # dataset nativo de torchtext

print(f'Number of training examples: {len(train_data)}')
print(f'Number of testing examples: {len(test_data)}')

print(vars(train_data.examples[50]))

import random
train_data, valid_data = train_data.split(random_state = random.seed(SEED))

print(f'Numero de training examples: {len(train_data)}')
print(f'Numero de validation examples: {len(valid_data)}')
# Test que previamente teniamos
print(f'Numero of testing examples: {len(test_data)}')

# Numero maximo de palabras que puede tener el vocabulario para optimizar procesamiento
MAX_VOCAB_SIZE = 25_000

# Por medio de TorchText le especificamos esta feature de vocabulario
#    Solo se especifica el de Train porque en el test solo debe reconocer el vocab. que se vio en el train
#.   si en dado caso no se conoce una palabra, se le remplaza como <unk>
TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)
LABEL.build_vocab(train_data)

print(f"Tokens unicos en el vocabulario de TEXT: {len(TEXT.vocab)}") 
# Almacena 2 mas por <unk> y <pad> para remplazar espacios en blanco donde la oracion 
# es mas peque単a que el tama単o del batch
print(f"Tokens unicos en el vocabulario de LABEL : {len(LABEL.vocab)}") # 1 - 0 ... bueno/malo

print(f"Tokens mas frecuentes: \n:{TEXT.vocab.freqs.most_common(20)}")

BATCH_SIZE = 64

# A cada dataset se le convierte en un tipo de iterador para que puede ser procesado por batches
train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(
                                                    (train_data, valid_data, test_data), 
                                                    batch_size = BATCH_SIZE,
                                                    device = device)

import torch.nn as nn

class RNN(nn.Module):
    #constructor
    # input_dim: dimension de los one-hot vectors (tama単o del vocabulario)
    # embedding_dim: dimension de las palabras en el texto (Depende del vocabulario)
    # hidden_dim : es del tama単o de los hidden states (depende de las dimensiones anteriores)
    # output_dim: es la ultima dimension (1-dim : 0 o 1 ) 
    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, embedding_dim)
        self.rnn = nn.RNN(embedding_dim, hidden_dim)
        self.fc = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, text):
        #text = [sent len, batch size]
        embedded = self.embedding(text)
        #embedded = [sent len, batch size, emb dim]
        output, hidden = self.rnn(embedded)
        #output = [sent len, batch size, hid dim]
        #hidden = [1, batch size, hid dim]
        
        assert torch.equal(output[-1,:,:], hidden.squeeze(0))
        
        return self.fc(hidden.squeeze(0))

INPUT_DIM = len(TEXT.vocab)
EMBEDDING_DIM = 100
HIDDEN_DIM = 256
OUTPUT_DIM = 1

model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)

import torch.optim as optim

# se utilizara Stochastic gradient descendant con learning rate de 1e-3
optimizer = optim.SGD(model.parameters(), lr=1e-3)

model.parameters

#Loss function - Sigmoid
loss = nn.BCEWithLogitsLoss()

model = model.to(device)
loss = loss.to(device)

def binary_accuracy(preds, y):
    """
    Retorna el accuracy de cada batch (ej. si se obtiene 8/10, devuelve 0.8, no 8
    """

    #round predictions to the closest integer
    rounded_preds = torch.round(torch.sigmoid(preds))
    correct = (rounded_preds == y).float() #convert into float for division 
    acc = correct.sum() / len(correct)
    return acc

def train(model, iterator, optimizer, loss):
    
    epoch_loss = 0
    epoch_acc = 0
    
    
    for batch in iterator:
        optimizer.zero_grad() # en cada epoch inicializa el gradiente en 0
        predictions = model(batch.text).squeeze(1) 
        lossT = loss(predictions, batch.label)
        acc = binary_accuracy(predictions, batch.label)
        
        lossT.backward() # calcula el gradiente
        optimizer.step() # hace update de los parametros
        
        epoch_loss += lossT.item() # extrae el valor del tensor 
        epoch_acc += acc.item()  #extrae el valor del tensor 
        
    return epoch_loss / len(iterator), epoch_acc / len(iterator)

#lo mismo que el train pero ahora no se calcularan los gradientes debido a que es para evaluar
# y no entrenar
def evaluate(model, iterator, criterion):
    
    epoch_loss = 0
    epoch_acc = 0
    
    
    with torch.no_grad():
        for batch in iterator:
            predictions = model(batch.text).squeeze(1)
            lossE = loss(predictions, batch.label)
            acc = binary_accuracy(predictions, batch.label)

            epoch_loss += lossE.item() # extrae el valor del tensor 
            epoch_acc += acc.item() #extrae el valor del tensor 
        
    return epoch_loss / len(iterator), epoch_acc / len(iterator)

N_EPOCHS = 100

# tipo 'inf' porque asi no se le condiciona a un rango max/min de valores u otro tipo con el 
# proposito de encontrar el valor de loss mas bajo
best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):

    
    train_loss, train_acc = train(model, train_iterator, optimizer, loss)
    valid_loss, valid_acc = evaluate(model, valid_iterator, loss)
    
    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
    
    print(f'Epoch: {epoch+1:02} ')
    print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')

test_loss, test_acc = evaluate(model, test_iterator, loss)

print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')

