{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgview(img, title = None):\n",
    "\n",
    "    fig = plt.figure(figsize = (5, 5))\n",
    "    plt.axis('off')    \n",
    "\n",
    "    plt.title(title) if title != None else None\n",
    "    \n",
    "    cmap_value = None if len(img.shape) == 3 else 'gray'\n",
    "\n",
    "    plt.imshow(img, cmap = cmap_value)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.MNIST('./data',train=True,download=True,transform=torchvision.transforms.ToTensor())\n",
    "test_data = torchvision.datasets.MNIST('./data',train=False,download=True,transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = 64\n",
    "learning_rate = 0.01\n",
    "n_epoch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data,batch_size = n_batch)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,batch_size = n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistCNN,self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        self.max1 = nn.MaxPool2d(2, 2)\n",
    "        self.cnn2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.max2 = nn.MaxPool2d(2, 2)\n",
    "        self.linear = nn.Linear(7*7*16,10)\n",
    "        self.relu = nn.ReLU()\n",
    "  \n",
    "    def forward(self,x):\n",
    "        n = x.size(0)\n",
    "        x = self.relu(self.cnn1(x))\n",
    "        x = self.relu(self.max1(x))\n",
    "        x = self.relu(self.cnn2(x))\n",
    "        x = self.relu(self.max2(x))\n",
    "        x = x.view(n,-1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistCNN().cuda()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epoch):\n",
    "    for batch_index, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        predictions = model(images)\n",
    "        loss = loss_function(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_index + 1) % 20 == 0:\n",
    "            # Validation\n",
    "            total, correct = 0, 0\n",
    "            for i, (images, labels) in enumerate(test_loader):\n",
    "                images = images.cuda()\n",
    "                x = model(images)\n",
    "                _, pred = torch.max(x,1)\n",
    "                pred = pred.data.cpu()\n",
    "                total += x.size(0)\n",
    "                correct_quantity = pred == labels\n",
    "                correct += torch.sum(correct_quantity)\n",
    "            accuracy = float(correct * 100 /total)\n",
    "            print('Epoch :',epoch + 1,'Batch :',batch_index + 1,'Loss :',float(loss.data),'Accuracy :',accuracy,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, \"checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistCNN()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"checkpoint\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iter(test_loader)\n",
    "\n",
    "def predict(img):\n",
    "    new_img = torch.tensor([[img]])\n",
    "    x = model(new_img)\n",
    "    _, pred = torch.max(x,1)\n",
    "    return int(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real: 3 , Predicted:  3\n"
     ]
    }
   ],
   "source": [
    "img, label = data.next()\n",
    "img = img[0, 0 ,:, :].numpy()\n",
    "imgview(img)\n",
    "print(\"Real:\",int(label[0]),\", Predicted: \", predict(img))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
